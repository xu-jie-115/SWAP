{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "668cc733-d308-45a4-839d-9bb24153d955",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 64 workers for parallel processing\n",
      "============================================================\n",
      "WIDTH UNCERTAINTY PROPAGATION ANALYSIS\n",
      "============================================================\n",
      "Pixel resolution: 30.0 m\n",
      "Width uncertainty: ±60.0 m (±1 pixel per bank)\n",
      "Monte Carlo samples: 100\n",
      "============================================================\n",
      "\n",
      "Loading common data...\n",
      "\n",
      "Total configurations to run: 1\n",
      "Configurations:\n",
      "  ('node', '1.5', 'noqa', 'VersionD')\n",
      "\n",
      "============================================================\n",
      "Running configuration: A=node, B=1.5, C=noqa, D=VersionD\n",
      "============================================================\n",
      "Uncertainty settings:\n",
      "  - Pixel resolution: 30.0m\n",
      "  - Width jitter per bank: ±30.0m\n",
      "  - Monte Carlo samples: 100\n",
      "Step 1: Calculating width statistics using IQR method...\n",
      "Skipped 28 stations due to small IQR (< 5):\n",
      "  Station Brazil_26800000: Q1=182.72, Q3=182.72, IQR=0.00\n",
      "  Station Brazil_31700000: Q1=111.73, Q3=115.16, IQR=3.43\n",
      "  Station Brazil_64795000: Q1=142.96, Q3=146.77, IQR=3.81\n",
      "  Station Canada_05BH004: Q1=98.55, Q3=102.17, IQR=3.62\n",
      "  Station Canada_05EF001: Q1=197.20, Q3=198.18, IQR=0.98\n",
      "  Station Canada_07EE007: Q1=103.99, Q3=108.82, IQR=4.83\n",
      "  Station Canada_07FB006: Q1=60.08, Q3=60.08, IQR=0.00\n",
      "  Station Canada_10EB001: Q1=119.95, Q3=120.14, IQR=0.19\n",
      "  Station Canada_10ED001: Q1=403.82, Q3=407.28, IQR=3.46\n",
      "  Station Canada_10HA004: Q1=90.57, Q3=90.57, IQR=0.00\n",
      "  ... and 18 more stations\n",
      "Step 2: Selecting best nodes...\n",
      "Original nodes: 4041, Selected nodes: 1766\n",
      "Step 3: Applying quality control...\n",
      "Step 4: Fitting hydraulic curves...\n",
      "Step 5: Generating hypsometric curves...\n",
      "Step 6: Validating model with uncertainty propagation...\n",
      "Step 7: Generating uncertainty analysis report...\n",
      "\n",
      "============================================================\n",
      "UNCERTAINTY ANALYSIS REPORT\n",
      "============================================================\n",
      "Total Stations: 558\n",
      "Valid Stations (no nan): 557\n",
      "Mean 95% Coverage: 0.4885\n",
      "Std 95% Coverage: 0.2701\n",
      "Stations with 95% Coverage >= 0.90: 45\n",
      "Stations with 95% Coverage >= 0.95: 16\n",
      "Mean 50% Coverage: 0.3087\n",
      "Std 50% Coverage: 0.2115\n",
      "Mean KGE: -1.0768\n",
      "Mean NSE: -40.5209\n",
      "Mean KGE (median): -1.0795\n",
      "Mean NSE (median): -40.9726\n",
      "Mean Relative CI Width (95%): 2.3992\n",
      "============================================================\n",
      "Configuration node_1.5_noqa_VersionD completed!\n",
      "Time for (node, 1.5, noqa, VersionD): 122.34s\n",
      "\n",
      "Generating boxplot comparisons...\n",
      "\n",
      "Generating summary comparison table...\n",
      "\n",
      "All configurations summary saved to: 7.all_configs_summary.csv\n",
      "\n",
      "Total time: 123.66s\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import linregress, spearmanr\n",
    "from scipy.optimize import least_squares\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import os\n",
    "import warnings\n",
    "import multiprocessing as mp\n",
    "from functools import partial\n",
    "import itertools\n",
    "from numba import jit, prange\n",
    "import gc\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ============================================================================\n",
    "# 全局配置\n",
    "# ============================================================================\n",
    "N_WORKERS = min(mp.cpu_count() - 1, 64)  # 80\n",
    "print(f\"Using {N_WORKERS} workers for parallel processing\")\n",
    "\n",
    "# ============================================================================\n",
    "# 【新增】不确定性传播配置\n",
    "# ============================================================================\n",
    "# 像素分辨率 (米)\n",
    "PIXEL_RESOLUTION = 30.0\n",
    "# 每岸抖动范围: ±1像素，两岸共 ±2像素 对宽度的影响\n",
    "WIDTH_JITTER_PER_BANK = PIXEL_RESOLUTION  # 30m per bank\n",
    "# 蒙特卡洛采样次数\n",
    "N_MC_SAMPLES = 100\n",
    "\n",
    "# ============================================================================\n",
    "# 全局变量（用于多进程共享数据）\n",
    "# ============================================================================\n",
    "_GLOBAL_NODE_DATA = {}\n",
    "_GLOBAL_WIDTH_STATS = None\n",
    "_GLOBAL_SWOT_DATA = None\n",
    "_GLOBAL_FITTER = None\n",
    "_GLOBAL_QC_DATA = None\n",
    "\n",
    "# ============================================================================\n",
    "# Numba加速的核心计算函数\n",
    "# ============================================================================\n",
    "@jit(nopython=True, parallel=True, cache=True)\n",
    "def compute_inconsistency_matrix(w, h):\n",
    "    \"\"\"使用Numba加速计算不一致性矩阵\"\"\"\n",
    "    n = len(w)\n",
    "    inverse = np.zeros(n, dtype=np.int64)\n",
    "    for i in prange(n):\n",
    "        count = 0\n",
    "        for j in range(n):\n",
    "            w_diff = w[i] - w[j]\n",
    "            h_diff = h[i] - h[j]\n",
    "            if w_diff * h_diff < 0:\n",
    "                count += 1\n",
    "        inverse[i] = count\n",
    "    return inverse\n",
    "\n",
    "@jit(nopython=True, cache=True)\n",
    "def calculate_areas_numba(w_list, h_list, w50, a50):\n",
    "    \"\"\"使用Numba加速面积计算\"\"\"\n",
    "    n = len(w_list)\n",
    "    areas = np.full(n, np.nan)\n",
    "    \n",
    "    # 边界检查：如果数据点太少，直接返回\n",
    "    if n < 2:\n",
    "        return areas\n",
    "    \n",
    "    idx50 = np.searchsorted(w_list, w50)\n",
    "    if idx50 >= n:\n",
    "        idx50 = n - 1\n",
    "    if idx50 < 1:\n",
    "        idx50 = 1\n",
    "    \n",
    "    # 防止除零错误\n",
    "    denom = w_list[idx50] - w_list[idx50-1]\n",
    "    if abs(denom) < 1e-10:\n",
    "        # 如果宽度差太小，使用平均值\n",
    "        h50 = (h_list[idx50-1] + h_list[idx50]) / 2.0\n",
    "    else:\n",
    "        h50 = (h_list[idx50-1] * (w_list[idx50] - w50) +\n",
    "               h_list[idx50] * (w50 - w_list[idx50-1])) / denom\n",
    "    \n",
    "    areas[idx50] = a50 + 0.5 * (w50 + w_list[idx50]) * (h_list[idx50] - h50)\n",
    "    \n",
    "    for i in range(idx50 + 1, n):\n",
    "        areas[i] = areas[i-1] + 0.5 * (w_list[i-1] + w_list[i]) * \\\n",
    "                  (h_list[i] - h_list[i-1])\n",
    "    \n",
    "    for i in range(idx50 - 1, -1, -1):\n",
    "        areas[i] = areas[i+1] - 0.5 * (w_list[i+1] + w_list[i]) * \\\n",
    "                  (h_list[i+1] - h_list[i])\n",
    "    \n",
    "    return areas\n",
    "\n",
    "@jit(nopython=True, cache=True)\n",
    "def nse_numba(observed, simulated):\n",
    "    \"\"\"Numba加速的NSE计算\"\"\"\n",
    "    obs_mean = np.mean(observed)\n",
    "    numerator = np.sum((observed - simulated)**2)\n",
    "    denominator = np.sum((observed - obs_mean)**2)\n",
    "    if denominator == 0:\n",
    "        return np.nan\n",
    "    return 1 - numerator / denominator\n",
    "\n",
    "@jit(nopython=True, cache=True)\n",
    "def kge_numba(observed, simulated):\n",
    "    \"\"\"Numba加速的KGE计算\"\"\"\n",
    "    obs_mean = np.mean(observed)\n",
    "    sim_mean = np.mean(simulated)\n",
    "    obs_std = np.std(observed)\n",
    "    sim_std = np.std(simulated)\n",
    "    \n",
    "    if obs_std == 0 or sim_std == 0 or obs_mean == 0 or sim_mean == 0:\n",
    "        return np.nan\n",
    "    \n",
    "    n = len(observed)\n",
    "    cov = np.sum((observed - obs_mean) * (simulated - sim_mean)) / n\n",
    "    r = cov / (obs_std * sim_std)\n",
    "    \n",
    "    alpha = sim_mean / obs_mean\n",
    "    beta = sim_std / obs_std\n",
    "    \n",
    "    return 1 - np.sqrt((r - 1)**2 + (alpha - 1)**2 + (beta - 1)**2)\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# 【新增】蒙特卡洛不确定性传播函数\n",
    "# ============================================================================\n",
    "@jit(nopython=True, cache=True)\n",
    "def monte_carlo_discharge_numba(width_obs, curve_width, curve_area, slope, \n",
    "                                 n_samples, width_jitter_range):\n",
    "    \"\"\"\n",
    "    使用Numba加速的蒙特卡洛流量不确定性计算\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    width_obs : float\n",
    "        观测宽度\n",
    "    curve_width : np.ndarray\n",
    "        曲线宽度数组\n",
    "    curve_area : np.ndarray\n",
    "        曲线面积数组\n",
    "    slope : float\n",
    "        河道坡降\n",
    "    n_samples : int\n",
    "        蒙特卡洛采样次数\n",
    "    width_jitter_range : float\n",
    "        宽度抖动范围 (两岸各1像素的不确定性，总范围 = 2*30 = 60m)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    q_samples : np.ndarray\n",
    "        流量采样数组\n",
    "    \"\"\"\n",
    "    q_samples = np.zeros(n_samples)\n",
    "    \n",
    "    for i in range(n_samples):\n",
    "        # 对宽度添加随机抖动\n",
    "        # 两岸各独立抖动 ±1 像素\n",
    "        # 使用均匀分布 [-jitter_range, +jitter_range] 模拟离散像素效应\n",
    "        jitter = (np.random.random() * 2.0 - 1.0) * width_jitter_range\n",
    "        w_jittered = width_obs + jitter\n",
    "        \n",
    "        # 确保宽度在曲线范围内\n",
    "        if w_jittered < curve_width[0]:\n",
    "            w_jittered = curve_width[0]\n",
    "        elif w_jittered > curve_width[-1]:\n",
    "            w_jittered = curve_width[-1]\n",
    "        \n",
    "        # 线性插值获取面积\n",
    "        idx = np.searchsorted(curve_width, w_jittered)\n",
    "        if idx == 0:\n",
    "            area = curve_area[0]\n",
    "        elif idx >= len(curve_width):\n",
    "            area = curve_area[-1]\n",
    "        else:\n",
    "            # 线性插值\n",
    "            w0, w1 = curve_width[idx-1], curve_width[idx]\n",
    "            a0, a1 = curve_area[idx-1], curve_area[idx]\n",
    "            if w1 - w0 > 0:\n",
    "                area = a0 + (a1 - a0) * (w_jittered - w0) / (w1 - w0)\n",
    "            else:\n",
    "                area = a0\n",
    "        \n",
    "        # 计算流量 (曼宁公式)\n",
    "        if area > 0 and w_jittered > 0 and slope > 0:\n",
    "            q_samples[i] = (area ** (5.0/3.0)) * (w_jittered ** (-2.0/3.0)) * (slope ** 0.5) / 0.035\n",
    "        else:\n",
    "            q_samples[i] = np.nan\n",
    "    \n",
    "    return q_samples\n",
    "\n",
    "\n",
    "@jit(nopython=True, cache=True)\n",
    "def monte_carlo_area_numba(width_obs, curve_width, curve_area, n_samples, width_jitter_range):\n",
    "    \"\"\"\n",
    "    使用Numba加速的蒙特卡洛面积不确定性计算\n",
    "    \"\"\"\n",
    "    area_samples = np.zeros(n_samples)\n",
    "    \n",
    "    for i in range(n_samples):\n",
    "        jitter = (np.random.random() * 2.0 - 1.0) * width_jitter_range\n",
    "        w_jittered = width_obs + jitter\n",
    "        \n",
    "        # 确保宽度在曲线范围内\n",
    "        if w_jittered < curve_width[0]:\n",
    "            w_jittered = curve_width[0]\n",
    "        elif w_jittered > curve_width[-1]:\n",
    "            w_jittered = curve_width[-1]\n",
    "        \n",
    "        # 线性插值获取面积\n",
    "        idx = np.searchsorted(curve_width, w_jittered)\n",
    "        if idx == 0:\n",
    "            area_samples[i] = curve_area[0]\n",
    "        elif idx >= len(curve_width):\n",
    "            area_samples[i] = curve_area[-1]\n",
    "        else:\n",
    "            w0, w1 = curve_width[idx-1], curve_width[idx]\n",
    "            a0, a1 = curve_area[idx-1], curve_area[idx]\n",
    "            if w1 - w0 > 0:\n",
    "                area_samples[i] = a0 + (a1 - a0) * (w_jittered - w0) / (w1 - w0)\n",
    "            else:\n",
    "                area_samples[i] = a0\n",
    "    \n",
    "    return area_samples\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# 全局并行处理函数（必须在模块级别定义才能被pickle）\n",
    "# ============================================================================\n",
    "def _compute_node_corr(node_id):\n",
    "    \"\"\"计算单个节点的秩相关系数\"\"\"\n",
    "    global _GLOBAL_NODE_DATA\n",
    "    data = _GLOBAL_NODE_DATA.get(node_id)\n",
    "    if data is None or len(data['width']) < 5:\n",
    "        return (node_id, data['stationid'] if data else None, 0.0)\n",
    "    \n",
    "    try:\n",
    "        corr, _ = spearmanr(data['width'], data['wse'])\n",
    "        if np.isnan(corr):\n",
    "            corr = 0.0\n",
    "    except:\n",
    "        corr = 0.0\n",
    "    \n",
    "    return (node_id, data['stationid'], corr)\n",
    "\n",
    "def _process_qc_station(stationid):\n",
    "    \"\"\"处理单个站点的质控（全局函数版本）\"\"\"\n",
    "    global _GLOBAL_WIDTH_STATS, _GLOBAL_SWOT_DATA\n",
    "    \n",
    "    if _GLOBAL_WIDTH_STATS is None or stationid not in _GLOBAL_WIDTH_STATS.index:\n",
    "        return None\n",
    "    \n",
    "    df = _GLOBAL_SWOT_DATA[_GLOBAL_SWOT_DATA['stationid'] == stationid].copy()\n",
    "    if len(df) < 5:\n",
    "        return None\n",
    "    \n",
    "    # 步骤1: 不确定度筛选\n",
    "    df['width_u_r'] = df['width_u'] / df['width']\n",
    "    df1 = df[(df['wse_u'] <= 0.4) & (df['width_u_r'] <= 0.1)]\n",
    "    if len(df1) < 5:\n",
    "        return None\n",
    "    \n",
    "    # 步骤2: 顺序一致性剔除\n",
    "    df2 = _remove_inconsistent_points(df1)\n",
    "    if len(df2) < 5:\n",
    "        return None\n",
    "    \n",
    "    # 步骤3: 离群值剔除\n",
    "    w_low = _GLOBAL_WIDTH_STATS.loc[stationid, 'w_low']\n",
    "    w_high = _GLOBAL_WIDTH_STATS.loc[stationid, 'w_high']\n",
    "    d_bankfull = 0.27 * (w_high / 7.2) ** 0.6\n",
    "    h50 = df2['wse'].median()\n",
    "    \n",
    "    df3 = df2[(df2['wse'] <= h50 + d_bankfull) & (df2['wse'] >= h50 - d_bankfull)]\n",
    "    \n",
    "    return df3 if len(df3) >= 5 else None\n",
    "\n",
    "def _remove_inconsistent_points(df, inverse_ratio_thresh=0.5):\n",
    "    \"\"\"顺序一致性剔除\"\"\"\n",
    "    indices_to_keep = list(df.index)\n",
    "    \n",
    "    while True:\n",
    "        n = len(indices_to_keep)\n",
    "        if n < 5:\n",
    "            break\n",
    "        \n",
    "        df_current = df.loc[indices_to_keep]\n",
    "        w = df_current['width'].values.astype(np.float64)\n",
    "        h = df_current['wse'].values.astype(np.float64)\n",
    "        \n",
    "        inverse = compute_inconsistency_matrix(w, h)\n",
    "        \n",
    "        idx_max = np.argmax(inverse)\n",
    "        if inverse[idx_max] / n < inverse_ratio_thresh:\n",
    "            break\n",
    "        \n",
    "        indices_to_keep.pop(idx_max)\n",
    "    \n",
    "    return df.loc[indices_to_keep]\n",
    "\n",
    "def _fit_station_wrapper(stationid):\n",
    "    \"\"\"拟合单个站点（全局函数版本）\"\"\"\n",
    "    global _GLOBAL_FITTER, _GLOBAL_QC_DATA\n",
    "    \n",
    "    df_station = _GLOBAL_QC_DATA[_GLOBAL_QC_DATA['stationid'] == stationid]\n",
    "    if len(df_station) == 0:\n",
    "        return None\n",
    "    \n",
    "    comid = df_station.iloc[0]['COMID']\n",
    "    return _GLOBAL_FITTER.fit_station(df_station, stationid, comid)\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# 【修改】验证函数 - 添加不确定性传播\n",
    "# ============================================================================\n",
    "def _validate_station_wrapper(args):\n",
    "    \"\"\"验证单个站点（全局函数版本）- 包含不确定性传播\"\"\"\n",
    "    # 【修改】添加 skip_width_filter 参数\n",
    "    s, df_hypso, df_width, df_val_folder, df_fit, start_date, skip_width_filter = args\n",
    "    \n",
    "    file_path = os.path.join(df_val_folder, f'{s}.csv')\n",
    "    if not os.path.exists(file_path):\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        df_val = pd.read_csv(file_path)\n",
    "        num_days = len(df_val)\n",
    "        df_val['date'] = pd.date_range(start=start_date, periods=num_days, freq='D')\n",
    "        df_val['stationid'] = s\n",
    "        df_val = df_val.dropna(subset=['qobs'])\n",
    "        \n",
    "        df_width_s = df_width[df_width['stationid'] == s]\n",
    "        df_val = df_val.merge(df_width_s, on=['stationid', 'date'], how='inner')\n",
    "        \n",
    "        df_curve = df_hypso[df_hypso['stationid'] == s].reset_index(drop=True)\n",
    "        station_fit = df_fit[df_fit['stationid'] == s]\n",
    "        if station_fit.empty or df_curve.empty:\n",
    "            return None\n",
    "        \n",
    "        row = station_fit.iloc[0]\n",
    "        w_low, w_high, slp = row['w_low'], row['w_high'], row['slp']\n",
    "        \n",
    "        # 【修改】根据 skip_width_filter 决定是否进行宽度筛选\n",
    "        if skip_width_filter:\n",
    "            # datemean模式：不对width做筛选，只去重\n",
    "            df_val = df_val.drop_duplicates('date')\n",
    "        else:\n",
    "            # node模式：正常进行宽度筛选\n",
    "            df_val = df_val[\n",
    "                (df_val['width'] >= w_low) &\n",
    "                (df_val['width'] <= w_high)\n",
    "            ].drop_duplicates('date')\n",
    "        \n",
    "        if len(df_val) < 10:\n",
    "            return None\n",
    "        \n",
    "        # 获取曲线数据\n",
    "        curve_width = df_curve['width'].values.astype(np.float64)\n",
    "        curve_area = df_curve['area'].values.astype(np.float64)\n",
    "        val_width = df_val['width'].values.astype(np.float64)\n",
    "        \n",
    "        # ================================================================\n",
    "        # 【新增】不确定性传播：蒙特卡洛模拟\n",
    "        # ================================================================\n",
    "        # 宽度不确定性：共抖动1像素\n",
    "        width_jitter_total = WIDTH_JITTER_PER_BANK  # 60m total range\n",
    "        \n",
    "        n_obs = len(df_val)\n",
    "        \n",
    "        # 存储结果\n",
    "        q_median = np.zeros(n_obs)\n",
    "        q_lower_95 = np.zeros(n_obs)  # 2.5th percentile\n",
    "        q_upper_95 = np.zeros(n_obs)  # 97.5th percentile\n",
    "        q_lower_50 = np.zeros(n_obs)  # 25th percentile\n",
    "        q_upper_50 = np.zeros(n_obs)  # 75th percentile\n",
    "        area_median = np.zeros(n_obs)\n",
    "        area_lower_95 = np.zeros(n_obs)\n",
    "        area_upper_95 = np.zeros(n_obs)\n",
    "        \n",
    "        # 对每个观测进行蒙特卡洛模拟\n",
    "        for i in range(n_obs):\n",
    "            w_obs = val_width[i]\n",
    "            \n",
    "            # 蒙特卡洛采样计算流量\n",
    "            q_samples = monte_carlo_discharge_numba(\n",
    "                w_obs, curve_width, curve_area, slp,\n",
    "                N_MC_SAMPLES, width_jitter_total\n",
    "            )\n",
    "            \n",
    "            # 蒙特卡洛采样计算面积\n",
    "            area_samples = monte_carlo_area_numba(\n",
    "                w_obs, curve_width, curve_area, \n",
    "                N_MC_SAMPLES, width_jitter_total\n",
    "            )\n",
    "            \n",
    "            # 计算流量统计量\n",
    "            q_valid = q_samples[~np.isnan(q_samples)]\n",
    "            if len(q_valid) > 0:\n",
    "                q_median[i] = np.median(q_valid)\n",
    "                q_lower_95[i] = np.percentile(q_valid, 2.5)\n",
    "                q_upper_95[i] = np.percentile(q_valid, 97.5)\n",
    "                q_lower_50[i] = np.percentile(q_valid, 25)\n",
    "                q_upper_50[i] = np.percentile(q_valid, 75)\n",
    "            else:\n",
    "                q_median[i] = np.nan\n",
    "                q_lower_95[i] = np.nan\n",
    "                q_upper_95[i] = np.nan\n",
    "                q_lower_50[i] = np.nan\n",
    "                q_upper_50[i] = np.nan\n",
    "            \n",
    "            # 计算面积统计量\n",
    "            area_valid = area_samples[~np.isnan(area_samples)]\n",
    "            if len(area_valid) > 0:\n",
    "                area_median[i] = np.median(area_valid)\n",
    "                area_lower_95[i] = np.percentile(area_valid, 2.5)\n",
    "                area_upper_95[i] = np.percentile(area_valid, 97.5)\n",
    "            else:\n",
    "                area_median[i] = np.nan\n",
    "                area_lower_95[i] = np.nan\n",
    "                area_upper_95[i] = np.nan\n",
    "        \n",
    "        # 原始点估计（不含不确定性）\n",
    "        area_hypso = np.interp(val_width, curve_width, curve_area)\n",
    "        q_est_point = (area_hypso**(5/3) * val_width**(-2/3) * slp**0.5 / 0.035)\n",
    "        \n",
    "        # 添加到数据框\n",
    "        df_val['area_hypso'] = area_hypso\n",
    "        df_val['Q_est'] = q_est_point\n",
    "        # 【新增】可信区间相关列\n",
    "        df_val['Q_est_median'] = q_median\n",
    "        df_val['Q_est_lower_95'] = q_lower_95\n",
    "        df_val['Q_est_upper_95'] = q_upper_95\n",
    "        df_val['Q_est_lower_50'] = q_lower_50\n",
    "        df_val['Q_est_upper_50'] = q_upper_50\n",
    "        df_val['area_median'] = area_median\n",
    "        df_val['area_lower_95'] = area_lower_95\n",
    "        df_val['area_upper_95'] = area_upper_95\n",
    "        \n",
    "        df_val = df_val.dropna(subset=['Q_est', 'qobs'])\n",
    "        if len(df_val) < 10:\n",
    "            return None\n",
    "        \n",
    "        obs = df_val['qobs'].values.astype(np.float64)\n",
    "        sim = df_val['Q_est'].values.astype(np.float64)\n",
    "        sim_median = df_val['Q_est_median'].values.astype(np.float64)\n",
    "        \n",
    "        # 计算性能指标（使用点估计）\n",
    "        kge_val = kge_numba(obs, sim)\n",
    "        nse_val = nse_numba(obs, sim)\n",
    "        rmse = np.sqrt(np.mean((obs - sim)**2))\n",
    "        nrmse_val = rmse / np.mean(obs)\n",
    "        \n",
    "        # 【新增】计算性能指标（使用中值估计）\n",
    "        kge_val_median = kge_numba(obs, sim_median)\n",
    "        nse_val_median = nse_numba(obs, sim_median)\n",
    "        \n",
    "        # ================================================================\n",
    "        # 【新增】计算覆盖率 (Coverage)\n",
    "        # ================================================================\n",
    "        # 95% 可信区间覆盖率\n",
    "        lower_95 = df_val['Q_est_lower_95'].values\n",
    "        upper_95 = df_val['Q_est_upper_95'].values\n",
    "        coverage_95 = np.mean((obs >= lower_95) & (obs <= upper_95))\n",
    "        \n",
    "        # 50% 可信区间覆盖率\n",
    "        lower_50 = df_val['Q_est_lower_50'].values\n",
    "        upper_50 = df_val['Q_est_upper_50'].values\n",
    "        coverage_50 = np.mean((obs >= lower_50) & (obs <= upper_50))\n",
    "        \n",
    "        # 【新增】计算可信区间宽度\n",
    "        ci_width_95 = np.mean(upper_95 - lower_95)\n",
    "        ci_width_50 = np.mean(upper_50 - lower_50)\n",
    "        mean_obs = np.mean(obs)\n",
    "        if mean_obs > 0 and np.isfinite(ci_width_95):\n",
    "            relative_ci_width_95 = ci_width_95 / mean_obs\n",
    "        else:\n",
    "            relative_ci_width_95 = np.nan\n",
    "        \n",
    "        # 添加指标到数据框\n",
    "        df_val['kge'] = kge_val\n",
    "        df_val['nse'] = nse_val\n",
    "        df_val['nrmse'] = nrmse_val\n",
    "        # 【新增】不确定性相关指标\n",
    "        df_val['kge_median'] = kge_val_median\n",
    "        df_val['nse_median'] = nse_val_median\n",
    "        df_val['coverage_95'] = coverage_95\n",
    "        df_val['coverage_50'] = coverage_50\n",
    "        df_val['ci_width_95'] = ci_width_95\n",
    "        df_val['ci_width_50'] = ci_width_50\n",
    "        df_val['relative_ci_width_95'] = relative_ci_width_95\n",
    "        \n",
    "        # 【修改】返回更多列\n",
    "        return df_val[['stationid', 'date', 'width', \n",
    "                       'area_hypso', 'area_median', 'area_lower_95', 'area_upper_95',\n",
    "                       'qobs', 'Q_est', 'Q_est_median', \n",
    "                       'Q_est_lower_95', 'Q_est_upper_95',\n",
    "                       'Q_est_lower_50', 'Q_est_upper_50',\n",
    "                       'kge', 'nse', 'nrmse',\n",
    "                       'kge_median', 'nse_median',\n",
    "                       'coverage_95', 'coverage_50',\n",
    "                       'ci_width_95', 'ci_width_50', 'relative_ci_width_95']]\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing station {s}: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "\n",
    "\n",
    "def _rolling_median_group(group):\n",
    "    \"\"\"滑动中值处理\"\"\"\n",
    "    group = group.sort_values('date')\n",
    "    group['width'] = group['width'].rolling(window=5, center=True, min_periods=1).median()\n",
    "    group['wse'] = group['wse'].rolling(window=5, center=True, min_periods=1).median()\n",
    "    return group\n",
    "\n",
    "# ============================================================================\n",
    "# 模块1: 数据统计工具 (修改为IQR方法)\n",
    "# ============================================================================\n",
    "class WidthStatistics:\n",
    "    \"\"\"计算河流宽度的统计特征 - 使用IQR方法\"\"\"\n",
    "    \n",
    "    # 定义IQR配置：{选项: IQR倍数}\n",
    "    # w_low = Q1 - k * IQR\n",
    "    # w_high = Q3 + k * IQR\n",
    "    IQR_CONFIG = {\n",
    "        '1.0': 1.0,\n",
    "        '1.5': 1.5,\n",
    "        '2.0': 2.0,\n",
    "        '2.5': 2.5,\n",
    "        '3.0': 3.0,\n",
    "        '4.0': 4.0\n",
    "    }\n",
    "    \n",
    "    @staticmethod\n",
    "    def calculate_width_iqr(df, min_width=30, valid_ratio=0.95, min_iqr=5):\n",
    "        \"\"\"\n",
    "        计算每个站点的宽度IQR范围\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        df : DataFrame\n",
    "            输入数据\n",
    "        min_width : float\n",
    "            最小有效宽度\n",
    "        valid_ratio : float\n",
    "            有效数据比例阈值\n",
    "        min_iqr : float\n",
    "            最小IQR阈值，当IQR小于此值时跳过该站点\n",
    "        \"\"\"\n",
    "        stationids = df['stationid'].unique()\n",
    "        result_data = []\n",
    "        skipped_stations = []\n",
    "        \n",
    "        for stationid in stationids:\n",
    "            station_data_all = df[df['stationid'] == stationid]['width'].dropna()\n",
    "            station_data = station_data_all[station_data_all >= min_width]\n",
    "            \n",
    "            if len(station_data_all) == 0:\n",
    "                continue\n",
    "            if len(station_data) / len(station_data_all) < valid_ratio:\n",
    "                continue\n",
    "            \n",
    "            if len(station_data) > 10:\n",
    "                w50 = station_data.median()\n",
    "                \n",
    "                # 计算Q1, Q3和IQR\n",
    "                q1 = station_data.quantile(0.25)\n",
    "                q3 = station_data.quantile(0.75)\n",
    "                iqr = q3 - q1\n",
    "                \n",
    "                # 检查IQR是否足够大，如果Q1和Q3太接近则跳过该站点\n",
    "                if iqr < min_iqr:\n",
    "                    skipped_stations.append((stationid, q1, q3, iqr))\n",
    "                    continue\n",
    "                \n",
    "                row_data = {\n",
    "                    'stationid': stationid,\n",
    "                    'w50': w50,\n",
    "                    'Q1': q1,\n",
    "                    'Q3': q3,\n",
    "                    'IQR': iqr\n",
    "                }\n",
    "                \n",
    "                # 动态计算所有IQR倍数配置的范围\n",
    "                for key, k in WidthStatistics.IQR_CONFIG.items():\n",
    "                    w_low = max(q1 - k * iqr, min_width)  # 确保不低于最小宽度\n",
    "                    w_high = q3 + k * iqr\n",
    "                    row_data[f'w_low_iqr{key}'] = w_low\n",
    "                    row_data[f'w_high_iqr{key}'] = w_high\n",
    "                \n",
    "                result_data.append(row_data)\n",
    "        \n",
    "        # 打印跳过的站点信息\n",
    "        if skipped_stations:\n",
    "            print(f\"Skipped {len(skipped_stations)} stations due to small IQR (< {min_iqr}):\")\n",
    "            for sid, q1, q3, iqr in skipped_stations[:10]:  # 只打印前10个\n",
    "                print(f\"  Station {sid}: Q1={q1:.2f}, Q3={q3:.2f}, IQR={iqr:.2f}\")\n",
    "            if len(skipped_stations) > 10:\n",
    "                print(f\"  ... and {len(skipped_stations) - 10} more stations\")\n",
    "        \n",
    "        return pd.DataFrame(result_data)\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_iqr_columns(b_option):\n",
    "        \"\"\"根据B选项获取对应的IQR列名\"\"\"\n",
    "        if b_option not in WidthStatistics.IQR_CONFIG:\n",
    "            raise ValueError(f\"Invalid B option: {b_option}. Valid options: {list(WidthStatistics.IQR_CONFIG.keys())}\")\n",
    "        \n",
    "        return f'w_low_iqr{b_option}', f'w_high_iqr{b_option}'\n",
    "    \n",
    "    # 保留旧方法以兼容（如果需要）\n",
    "    @staticmethod\n",
    "    def calculate_width_percentiles(df, min_width=30, valid_ratio=0.95, min_iqr=5):\n",
    "        \"\"\"计算每个站点的宽度分位数（保留用于兼容）\"\"\"\n",
    "        return WidthStatistics.calculate_width_iqr(df, min_width, valid_ratio, min_iqr)\n",
    "\n",
    "# ============================================================================\n",
    "# 模块2: 节点选择\n",
    "# ============================================================================\n",
    "class NodeSelector:\n",
    "    \"\"\"为每个站点选择最优节点\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def select_best_nodes(df_swot, min_data_points=10):\n",
    "        \"\"\"为每个站点选择秩相关系数最大的节点\"\"\"\n",
    "        global _GLOBAL_NODE_DATA\n",
    "        \n",
    "        # 向量化计算节点数据量\n",
    "        node_counts = df_swot.groupby('node_id').size()\n",
    "        valid_nodes = node_counts[node_counts >= min_data_points].index\n",
    "        df_swot = df_swot[df_swot['node_id'].isin(valid_nodes)].copy()\n",
    "        \n",
    "        # 预计算每个节点的数据\n",
    "        _GLOBAL_NODE_DATA = {}\n",
    "        for node_id, group in df_swot.groupby('node_id'):\n",
    "            _GLOBAL_NODE_DATA[node_id] = {\n",
    "                'width': group['width'].values,\n",
    "                'wse': group['wse'].values,\n",
    "                'stationid': group['stationid'].iloc[0]\n",
    "            }\n",
    "        \n",
    "        node_ids = list(_GLOBAL_NODE_DATA.keys())\n",
    "        \n",
    "        # 使用进程池并行计算\n",
    "        with mp.Pool(processes=N_WORKERS) as pool:\n",
    "            results = pool.map(_compute_node_corr, node_ids)\n",
    "        \n",
    "        df_node = pd.DataFrame(results, columns=['node_id', 'stationid', 'rank_corr'])\n",
    "        df_node = df_node.dropna(subset=['stationid'])\n",
    "        \n",
    "        # 选择每个站点的最大秩相关系数节点\n",
    "        max_idx = df_node.groupby('stationid')['rank_corr'].idxmax()\n",
    "        df_node_rmax = df_node.loc[max_idx]\n",
    "        \n",
    "        # 筛选数据\n",
    "        df_filtered = df_swot[df_swot['node_id'].isin(df_node_rmax['node_id'])]\n",
    "        \n",
    "        print(f\"Original nodes: {len(df_node)}, Selected nodes: {len(df_node_rmax)}\")\n",
    "        \n",
    "        # 清理全局变量\n",
    "        _GLOBAL_NODE_DATA = {}\n",
    "        \n",
    "        return df_filtered, df_node_rmax\n",
    "\n",
    "# ============================================================================\n",
    "# 模块3: 数据质控\n",
    "# ============================================================================\n",
    "class DataQualityControl:\n",
    "    \"\"\"SWOT数据质量控制\"\"\"\n",
    "    \n",
    "    def __init__(self, width_stats):\n",
    "        self.width_stats = width_stats.set_index('stationid')\n",
    "    \n",
    "    def apply_qc(self, df_swot, draw_figure=False, output_folder=None):\n",
    "        \"\"\"应用完整的质量控制流程\"\"\"\n",
    "        global _GLOBAL_WIDTH_STATS, _GLOBAL_SWOT_DATA\n",
    "        \n",
    "        _GLOBAL_WIDTH_STATS = self.width_stats\n",
    "        _GLOBAL_SWOT_DATA = df_swot\n",
    "        \n",
    "        stationids = df_swot['stationid'].unique()\n",
    "        \n",
    "        # 使用进程池并行处理\n",
    "        with mp.Pool(processes=N_WORKERS) as pool:\n",
    "            results = pool.map(_process_qc_station, stationids)\n",
    "        \n",
    "        # 清理全局变量\n",
    "        _GLOBAL_WIDTH_STATS = None\n",
    "        _GLOBAL_SWOT_DATA = None\n",
    "        \n",
    "        # 合并结果\n",
    "        results = [df for df in results if df is not None]\n",
    "        \n",
    "        if not results:\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        result = pd.concat(results)\n",
    "        result = result.drop_duplicates(subset=['node_id', 'date', 'stationid'])\n",
    "        result.reset_index(drop=True, inplace=True)\n",
    "        \n",
    "        return result\n",
    "\n",
    "# ============================================================================\n",
    "# 模块4: 水位-面积曲线拟合\n",
    "# ============================================================================\n",
    "class HydraulicCurveFitter:\n",
    "    \"\"\"拟合水位-宽度关系曲线\"\"\"\n",
    "    \n",
    "    def __init__(self, width_stats, river_attrs, skip_width_filter=False):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "        -----------\n",
    "        width_stats : DataFrame\n",
    "            宽度统计数据\n",
    "        river_attrs : DataFrame\n",
    "            河流属性数据\n",
    "        skip_width_filter : bool\n",
    "            是否跳过宽度筛选（用于datemean模式）\n",
    "        \"\"\"\n",
    "        self.width_stats = width_stats.set_index('stationid')\n",
    "        self.river_attrs = river_attrs.set_index('COMID')\n",
    "        self.skip_width_filter = skip_width_filter\n",
    "        \n",
    "        self.R_list = np.array([0.5, 1, 2, 4, 8])\n",
    "        self.GAP_list = np.array([-0.3,-0.1,0,0.1,0.3])\n",
    "        self.W_list = np.array([0.3, 0.5, 0.7])\n",
    "    \n",
    "    @staticmethod\n",
    "    def power_function(params, X, y):\n",
    "        wse0, a, b = params\n",
    "        return y - (wse0 + a * X**b)\n",
    "    \n",
    "    def loss_function(self, z, weight, n_swot):\n",
    "        rho = np.zeros((3, len(z)))\n",
    "        rho[0] = 2 * ((1 + z)**0.5 - 1)\n",
    "        rho[1] = (1 + z)**(-0.5)\n",
    "        rho[2] = -0.5 * (1 + z)**(-1.5)\n",
    "        \n",
    "        factor = (n_swot - 2) / weight * (1 - weight) / 2\n",
    "        rho[:, 0] *= factor\n",
    "        rho[:, 1] *= factor\n",
    "        \n",
    "        return rho\n",
    "    \n",
    "    def calculate_h50(self, df, w50):\n",
    "        df = df.copy()\n",
    "        df['w50_diff'] = np.abs(df['width'] - w50)\n",
    "        df = df.sort_values('w50_diff')\n",
    "        \n",
    "        xdata = df.iloc[:5]['width'].values\n",
    "        ydata = df.iloc[:5]['wse'].values\n",
    "        xdata_uni = np.unique(xdata)\n",
    "        \n",
    "        if len(xdata_uni) < 2:\n",
    "            return df.iloc[:5]['wse'].mean()\n",
    "        \n",
    "        res = linregress(xdata, ydata)\n",
    "        if res[0] >= 0:\n",
    "            return res[0] * w50 + res[1]\n",
    "        else:\n",
    "            return df.iloc[:5]['wse'].mean()\n",
    "    \n",
    "    def fit_station(self, df_station, stationid, comid):\n",
    "        \"\"\"拟合单个站点的水位-宽度关系\"\"\"\n",
    "        if stationid not in self.width_stats.index:\n",
    "            return None\n",
    "        if comid not in self.river_attrs.index:\n",
    "            return None\n",
    "        \n",
    "        try:\n",
    "            q50 = self.river_attrs.loc[comid, 'q50_weighted']\n",
    "            slp = self.river_attrs.loc[comid, 'slope']\n",
    "            w50, w_low, w_high = self.width_stats.loc[stationid, ['w50', 'w_low', 'w_high']]\n",
    "            d_bankfull = 0.27 * (w_high / 7.2)**0.6\n",
    "            \n",
    "            h50 = self.calculate_h50(df_station, w50)\n",
    "            a50 = (q50 * 0.035 / slp**0.5 * w50**(2/3))**(3/5)\n",
    "            \n",
    "            # 根据skip_width_filter决定是否进行宽度筛选\n",
    "            if self.skip_width_filter:\n",
    "                # datemean模式：不对width做筛选\n",
    "                df_filtered = df_station.copy()\n",
    "            else:\n",
    "                # node模式：正常进行宽度筛选\n",
    "                df_filtered = df_station[\n",
    "                    (df_station['width'] >= w_low) &\n",
    "                    (df_station['width'] <= w_high)\n",
    "                ]\n",
    "            \n",
    "            if len(df_filtered) < 3:\n",
    "                return None\n",
    "            \n",
    "            swot_wsemax = df_filtered.sort_values('wse', ascending=False).iloc[0]\n",
    "            d_wsemax = 0.27 * (swot_wsemax['width'] / 7.2)**0.6\n",
    "            \n",
    "            results = []\n",
    "            for r_low in self.R_list:\n",
    "                for gap in self.GAP_list:\n",
    "                    for weight in self.W_list:\n",
    "                        result = self._fit_single_config(\n",
    "                            df_filtered, r_low, gap, weight,\n",
    "                            w_low, w_high, w50, h50, a50,\n",
    "                            swot_wsemax, d_bankfull, d_wsemax, slp, q50\n",
    "                        )\n",
    "                        if result is not None:\n",
    "                            result.update({\n",
    "                                'stationid': stationid,\n",
    "                                'COMID': comid,\n",
    "                                'R': r_low,\n",
    "                                'GAP': gap,\n",
    "                                'W': weight\n",
    "                            })\n",
    "                            results.append(result)\n",
    "            \n",
    "            return pd.DataFrame(results) if results else None\n",
    "        except Exception as e:\n",
    "            return None\n",
    "    \n",
    "    def _fit_single_config(self, df, r_low, gap, weight, w_low, w_high, w50,\n",
    "                          h50, a50, swot_wsemax, d_bankfull, d_wsemax, slp, q50):\n",
    "        \"\"\"拟合单个参数配置\"\"\"\n",
    "        a_low = a50 * (r_low + 1) / r_low / w50**(r_low + 1)\n",
    "        h0 = h50 - a_low * w50**r_low\n",
    "        h_low = h0 + a_low * w_low**r_low\n",
    "        h_high = swot_wsemax['wse'] + (d_bankfull - d_wsemax) + gap * d_bankfull\n",
    "        \n",
    "        xdata = np.insert(df['width'].values, 0, [w_low, w_high])\n",
    "        ydata = np.insert(df['wse'].values, 0, [h_low, h_high])\n",
    "        a_default = (h_high - h0) / w_high**2\n",
    "        \n",
    "        n_swot = len(df)\n",
    "        \n",
    "        def loss_wrapper(z):\n",
    "            return self.loss_function(z, weight, n_swot)\n",
    "        \n",
    "        try:\n",
    "            ls = least_squares(\n",
    "                self.power_function,\n",
    "                x0=[h0, a_default, 2],\n",
    "                loss=loss_wrapper,\n",
    "                args=(xdata, ydata),\n",
    "                max_nfev=100\n",
    "            )\n",
    "            \n",
    "            if ls.status > 0:\n",
    "                wse0, a, b = ls.x\n",
    "                if a * b < 0:\n",
    "                    return None\n",
    "                \n",
    "                return {\n",
    "                    'wse0': wse0, 'a': a, 'b': b,\n",
    "                    'a50': a50, 'w50': w50, 'q50': q50,\n",
    "                    'w_low': w_low, 'w_high': w_high,\n",
    "                    'h_low': h_low, 'h_high': h_high,\n",
    "                    'slp': slp\n",
    "                }\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    def fit_all_stations(self, df_qc):\n",
    "        \"\"\"并行拟合所有站点\"\"\"\n",
    "        global _GLOBAL_FITTER, _GLOBAL_QC_DATA\n",
    "        \n",
    "        _GLOBAL_FITTER = self\n",
    "        _GLOBAL_QC_DATA = df_qc\n",
    "        \n",
    "        unique_stations = df_qc['stationid'].unique()\n",
    "        \n",
    "        # 使用进程池并行处理\n",
    "        with mp.Pool(processes=N_WORKERS) as pool:\n",
    "            results = pool.map(_fit_station_wrapper, unique_stations)\n",
    "        \n",
    "        # 清理全局变量\n",
    "        _GLOBAL_FITTER = None\n",
    "        _GLOBAL_QC_DATA = None\n",
    "        \n",
    "        results = [df for df in results if df is not None]\n",
    "        \n",
    "        if results:\n",
    "            return pd.concat(results, ignore_index=True)\n",
    "        return None\n",
    "\n",
    "# ============================================================================\n",
    "# 模块5: 水位-面积曲线生成\n",
    "# ============================================================================\n",
    "class HypsometricCurveGenerator:\n",
    "    \"\"\"生成水位-面积关系曲线\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def generate_curves(df_fit, n_points=100):\n",
    "        \"\"\"为所有站点生成中值水位-面积曲线\"\"\"\n",
    "        stationids = sorted(df_fit['stationid'].unique())\n",
    "        df_res = []\n",
    "        \n",
    "        for s in stationids:\n",
    "            df_station = df_fit[df_fit['stationid'] == s]\n",
    "            w_low, w_high, w50, a50 = df_station.iloc[0][\n",
    "                ['w_low', 'w_high', 'w50', 'a50']\n",
    "            ]\n",
    "            \n",
    "            # 边界检查：跳过无效的宽度范围\n",
    "            if w_high <= w_low or abs(w_high - w_low) < 1e-6:\n",
    "                print(f\"Warning: Skipping station {s} due to invalid width range (w_low={w_low}, w_high={w_high})\")\n",
    "                continue\n",
    "            \n",
    "            wse0 = df_station['wse0'].values\n",
    "            a = df_station['a'].values\n",
    "            b = df_station['b'].values\n",
    "            \n",
    "            w_list = np.linspace(w_low, w_high, n_points)\n",
    "            \n",
    "            # 向量化计算\n",
    "            heights_all = wse0[:, np.newaxis] + a[:, np.newaxis] * w_list**b[:, np.newaxis]\n",
    "            h_list = np.median(heights_all, axis=0)\n",
    "            hmax = np.max(heights_all, axis=0)\n",
    "            hmin = np.min(heights_all, axis=0)\n",
    "            \n",
    "            # Numba加速的面积计算\n",
    "            areas = calculate_areas_numba(w_list, h_list, w50, a50)\n",
    "            \n",
    "            df_curve = pd.DataFrame({\n",
    "                'stationid': s,\n",
    "                'width': w_list,\n",
    "                'wse': h_list,\n",
    "                'wse_max': hmax,\n",
    "                'wse_min': hmin,\n",
    "                'area': areas\n",
    "            })\n",
    "            \n",
    "            df_res.append(df_curve)\n",
    "        \n",
    "        return pd.concat(df_res, ignore_index=True) if df_res else pd.DataFrame()\n",
    "\n",
    "# ============================================================================\n",
    "# 模块6: 验证与评估\n",
    "# ============================================================================\n",
    "class ModelValidator:\n",
    "    \"\"\"模型验证与性能评估\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def relative_rmse(observed, simulated):\n",
    "        rmse = np.sqrt(mean_squared_error(observed, simulated))\n",
    "        return rmse / np.mean(observed)\n",
    "    \n",
    "    def validate(self, df_hypso, df_width, df_val_folder, df_fit, skip_width_filter=False):\n",
    "        \"\"\"\n",
    "        验证模型性能\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        skip_width_filter : bool\n",
    "            是否跳过宽度筛选（用于datemean模式）\n",
    "        \"\"\"\n",
    "        stationids = sorted(df_hypso['stationid'].unique())\n",
    "        start_date = pd.to_datetime('1979-01-01')\n",
    "        \n",
    "        # 【修改】在参数列表中添加 skip_width_filter\n",
    "        args_list = [\n",
    "            (s, df_hypso, df_width, df_val_folder, df_fit, start_date, skip_width_filter)\n",
    "            for s in stationids\n",
    "        ]\n",
    "        \n",
    "        # 使用进程池并行处理\n",
    "        with mp.Pool(processes=N_WORKERS) as pool:\n",
    "            results = pool.map(_validate_station_wrapper, args_list)\n",
    "        \n",
    "        results = [df for df in results if df is not None]\n",
    "        \n",
    "        return pd.concat(results, ignore_index=True) if results else pd.DataFrame()\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# 【新增】模块7: 不确定性可视化\n",
    "# ============================================================================\n",
    "class UncertaintyVisualizer:\n",
    "    \"\"\"不确定性结果可视化\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def plot_coverage_summary(df_results, output_path):\n",
    "        \"\"\"绘制覆盖率汇总图\"\"\"\n",
    "        # 按站点汇总\n",
    "        station_summary = df_results.groupby('stationid').agg({\n",
    "            'coverage_95': 'first',\n",
    "            'coverage_50': 'first',\n",
    "            'kge': 'first',\n",
    "            'nse': 'first',\n",
    "            'relative_ci_width_95': 'first'\n",
    "        }).reset_index()\n",
    "        \n",
    "        # 过滤掉包含 inf 或 nan 的行\n",
    "        station_summary = station_summary.replace([np.inf, -np.inf], np.nan)\n",
    "        station_summary_clean = station_summary.dropna()\n",
    "        \n",
    "        fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "        \n",
    "        # 1. 95%覆盖率分布\n",
    "        ax1 = axes[0, 0]\n",
    "        coverage_95_valid = station_summary_clean['coverage_95'].dropna()\n",
    "        if len(coverage_95_valid) > 0:\n",
    "            ax1.hist(coverage_95_valid, bins=20, edgecolor='black', alpha=0.7)\n",
    "            ax1.axvline(x=0.95, color='red', linestyle='--', label='Expected (95%)')\n",
    "            ax1.axvline(x=coverage_95_valid.mean(), color='blue', \n",
    "                       linestyle='-', label=f\"Mean ({coverage_95_valid.mean():.2f})\")\n",
    "            ax1.legend()\n",
    "        ax1.set_xlabel('95% CI Coverage')\n",
    "        ax1.set_ylabel('Number of Stations')\n",
    "        ax1.set_title('95% Credible Interval Coverage')\n",
    "        \n",
    "        # 2. 50%覆盖率分布\n",
    "        ax2 = axes[0, 1]\n",
    "        coverage_50_valid = station_summary_clean['coverage_50'].dropna()\n",
    "        if len(coverage_50_valid) > 0:\n",
    "            ax2.hist(coverage_50_valid, bins=20, edgecolor='black', alpha=0.7)\n",
    "            ax2.axvline(x=0.50, color='red', linestyle='--', label='Expected (50%)')\n",
    "            ax2.axvline(x=coverage_50_valid.mean(), color='blue',\n",
    "                       linestyle='-', label=f\"Mean ({coverage_50_valid.mean():.2f})\")\n",
    "            ax2.legend()\n",
    "        ax2.set_xlabel('50% CI Coverage')\n",
    "        ax2.set_ylabel('Number of Stations')\n",
    "        ax2.set_title('50% Credible Interval Coverage')\n",
    "        \n",
    "        # 3. 覆盖率 vs KGE\n",
    "        ax3 = axes[1, 0]\n",
    "        # 过滤有效数据用于散点图\n",
    "        scatter_data = station_summary_clean[\n",
    "            station_summary_clean['kge'].notna() & \n",
    "            station_summary_clean['coverage_95'].notna() &\n",
    "            station_summary_clean['relative_ci_width_95'].notna() &\n",
    "            np.isfinite(station_summary_clean['relative_ci_width_95'])\n",
    "        ]\n",
    "        if len(scatter_data) > 0:\n",
    "            scatter = ax3.scatter(scatter_data['kge'], scatter_data['coverage_95'],\n",
    "                                 c=scatter_data['relative_ci_width_95'], cmap='viridis',\n",
    "                                 alpha=0.6)\n",
    "            ax3.axhline(y=0.95, color='red', linestyle='--', alpha=0.5)\n",
    "            plt.colorbar(scatter, ax=ax3, label='Relative CI Width')\n",
    "        ax3.set_xlabel('KGE')\n",
    "        ax3.set_ylabel('95% CI Coverage')\n",
    "        ax3.set_title('Coverage vs Performance')\n",
    "        \n",
    "        # 4. CI宽度分布\n",
    "        ax4 = axes[1, 1]\n",
    "        ci_width_valid = station_summary_clean['relative_ci_width_95'].dropna()\n",
    "        ci_width_valid = ci_width_valid[np.isfinite(ci_width_valid)]\n",
    "        if len(ci_width_valid) > 0:\n",
    "            ax4.hist(ci_width_valid, bins=20, edgecolor='black', alpha=0.7)\n",
    "        ax4.set_xlabel('Relative 95% CI Width (CI/mean Q)')\n",
    "        ax4.set_ylabel('Number of Stations')\n",
    "        ax4.set_title('Uncertainty Width Distribution')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(output_path, dpi=150)\n",
    "        plt.close()\n",
    "        \n",
    "        return station_summary\n",
    "    \n",
    "    @staticmethod\n",
    "    def plot_station_timeseries(df_station, stationid, output_path):\n",
    "        \"\"\"绘制单站点时序图，包含可信区间\"\"\"\n",
    "        fig, ax = plt.subplots(figsize=(14, 6))\n",
    "        \n",
    "        dates = pd.to_datetime(df_station['date'])\n",
    "        \n",
    "        # 95% 可信区间\n",
    "        ax.fill_between(dates, df_station['Q_est_lower_95'], df_station['Q_est_upper_95'],\n",
    "                       alpha=0.2, color='blue', label='95% CI')\n",
    "        \n",
    "        # 50% 可信区间\n",
    "        ax.fill_between(dates, df_station['Q_est_lower_50'], df_station['Q_est_upper_50'],\n",
    "                       alpha=0.4, color='blue', label='50% CI')\n",
    "        \n",
    "        # 中值估计\n",
    "        ax.plot(dates, df_station['Q_est_median'], 'b-', linewidth=1, label='Estimated Q (median)')\n",
    "        \n",
    "        # 观测值\n",
    "        ax.plot(dates, df_station['qobs'], 'ko', markersize=3, alpha=0.6, label='Observed Q')\n",
    "        \n",
    "        # 标注覆盖率\n",
    "        coverage_95 = df_station['coverage_95'].iloc[0]\n",
    "        coverage_50 = df_station['coverage_50'].iloc[0]\n",
    "        kge = df_station['kge'].iloc[0]\n",
    "        \n",
    "        ax.set_xlabel('Date')\n",
    "        ax.set_ylabel('Discharge (m³/s)')\n",
    "        ax.set_title(f'Station {stationid}\\nKGE={kge:.2f}, 95% Coverage={coverage_95:.2f}, 50% Coverage={coverage_50:.2f}')\n",
    "        ax.legend(loc='upper right')\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(output_path, dpi=150)\n",
    "        plt.close()\n",
    "    \n",
    "    @staticmethod\n",
    "    def create_coverage_report(df_results, output_path):\n",
    "        \"\"\"生成覆盖率报告\"\"\"\n",
    "        # 按站点汇总\n",
    "        station_summary = df_results.groupby('stationid').agg({\n",
    "            'coverage_95': 'first',\n",
    "            'coverage_50': 'first',\n",
    "            'kge': 'first',\n",
    "            'nse': 'first',\n",
    "            'nrmse': 'first',\n",
    "            'kge_median': 'first',\n",
    "            'nse_median': 'first',\n",
    "            'ci_width_95': 'first',\n",
    "            'ci_width_50': 'first',\n",
    "            'relative_ci_width_95': 'first',\n",
    "            'qobs': 'mean'\n",
    "        }).reset_index()\n",
    "        \n",
    "        station_summary.columns = ['stationid', 'coverage_95', 'coverage_50', \n",
    "                                   'kge', 'nse', 'nrmse', 'kge_median', 'nse_median',\n",
    "                                   'ci_width_95', 'ci_width_50', 'relative_ci_width_95',\n",
    "                                   'mean_qobs']\n",
    "        \n",
    "        # 替换 inf 为 nan，便于统计\n",
    "        station_summary = station_summary.replace([np.inf, -np.inf], np.nan)\n",
    "        \n",
    "        # 计算汇总统计（忽略nan）\n",
    "        report = {\n",
    "            'Total Stations': len(station_summary),\n",
    "            'Valid Stations (no nan)': station_summary.dropna().shape[0],\n",
    "            'Mean 95% Coverage': station_summary['coverage_95'].mean(skipna=True),\n",
    "            'Std 95% Coverage': station_summary['coverage_95'].std(skipna=True),\n",
    "            'Stations with 95% Coverage >= 0.90': (station_summary['coverage_95'] >= 0.90).sum(),\n",
    "            'Stations with 95% Coverage >= 0.95': (station_summary['coverage_95'] >= 0.95).sum(),\n",
    "            'Mean 50% Coverage': station_summary['coverage_50'].mean(skipna=True),\n",
    "            'Std 50% Coverage': station_summary['coverage_50'].std(skipna=True),\n",
    "            'Mean KGE': station_summary['kge'].mean(skipna=True),\n",
    "            'Mean NSE': station_summary['nse'].mean(skipna=True),\n",
    "            'Mean KGE (median)': station_summary['kge_median'].mean(skipna=True),\n",
    "            'Mean NSE (median)': station_summary['nse_median'].mean(skipna=True),\n",
    "            'Mean Relative CI Width (95%)': station_summary['relative_ci_width_95'].mean(skipna=True),\n",
    "        }\n",
    "        \n",
    "        # 保存报告\n",
    "        report_df = pd.DataFrame([report])\n",
    "        report_df.to_csv(output_path.replace('.csv', '_summary.csv'), index=False)\n",
    "        station_summary.to_csv(output_path, index=False)\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"UNCERTAINTY ANALYSIS REPORT\")\n",
    "        print(\"=\"*60)\n",
    "        for key, value in report.items():\n",
    "            if isinstance(value, float):\n",
    "                print(f\"{key}: {value:.4f}\")\n",
    "            else:\n",
    "                print(f\"{key}: {value}\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        return station_summary, report\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# 配置运行函数\n",
    "# ============================================================================\n",
    "def run_configuration(a, b, c, d, common_data):\n",
    "    \"\"\"\n",
    "    运行单个配置\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    a : str\n",
    "        处理方式: 'node' 或 'datemean'\n",
    "    b : str\n",
    "        IQR倍数选项: '1.0', '1.5', '2.0', '2.5', '3.0', '4.0'\n",
    "    c : str\n",
    "        QA选项: 'noqa', 'qaloose', 'qastrict'\n",
    "    d : str\n",
    "        版本选项: 'VersionD', 'VersionC'\n",
    "    common_data : dict\n",
    "        共享数据\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Running configuration: A={a}, B={b}, C={c}, D={d}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    # 【新增】打印不确定性配置\n",
    "    print(f\"Uncertainty settings:\")\n",
    "    print(f\"  - Pixel resolution: {PIXEL_RESOLUTION}m\")\n",
    "    print(f\"  - Width jitter per bank: ±{WIDTH_JITTER_PER_BANK}m\")\n",
    "    print(f\"  - Monte Carlo samples: {N_MC_SAMPLES}\")\n",
    "    \n",
    "    gc.collect()\n",
    "    \n",
    "    # 步骤1: 计算宽度统计（使用IQR方法）\n",
    "    print(\"Step 1: Calculating width statistics using IQR method...\")\n",
    "    df_l8 = common_data['df_l8']\n",
    "    df_w_stats = WidthStatistics.calculate_width_iqr(df_l8)\n",
    "    \n",
    "    # 使用IQR方法获取列名\n",
    "    low_col, high_col = WidthStatistics.get_iqr_columns(b)\n",
    "    df_w_stats['w_low'] = df_w_stats[low_col]\n",
    "    df_w_stats['w_high'] = df_w_stats[high_col]\n",
    "    \n",
    "    # 【修改】添加\"_0.3\"后缀\n",
    "    df_w_stats.to_csv(f'1.width_statistic_iqr_{a}_{b}_{c}_{d}.csv', index=False)\n",
    "    \n",
    "    df_comid = common_data['df_comid']\n",
    "    df_attrs = common_data['df_attrs']\n",
    "\n",
    "    # 根据c选项加载SWOT数据\n",
    "    if c == 'noqa':\n",
    "        # noqa时文件名固定\n",
    "        df_swot = pd.read_csv(f'1.all_matched_points_{d}.csv')\n",
    "    else:\n",
    "        # qaloose或qastrict时，文件名根据a确定\n",
    "        df_swot = pd.read_csv(f'2.swot_{a}_{c}_{d}.csv')\n",
    "   \n",
    "    df_swot = df_swot.merge(df_comid, on='stationid', how='inner')\n",
    "\n",
    "    if a == 'node':\n",
    "        print(\"Step 2: Selecting best nodes...\")\n",
    "        df_swot_filtered, df_nodes = NodeSelector.select_best_nodes(df_swot, min_data_points=10)\n",
    "        \n",
    "        print(\"Step 3: Applying quality control...\")\n",
    "        qc = DataQualityControl(df_w_stats)\n",
    "        df_qc = qc.apply_qc(df_swot_filtered, draw_figure=False)\n",
    "        \n",
    "    elif a == 'datemean':\n",
    "        print(\"Using smoothed data (skipping width filter)...\")\n",
    "        # 使用进程池并行处理滑动中值\n",
    "        groups = [group for _, group in df_swot.groupby('stationid')]\n",
    "        \n",
    "        with mp.Pool(processes=N_WORKERS) as pool:\n",
    "            results = pool.map(_rolling_median_group, groups)\n",
    "        \n",
    "        df_qc = pd.concat(results)\n",
    "    \n",
    "    if 'COMID' not in df_qc.columns:\n",
    "        df_qc = df_qc.merge(df_comid, on='stationid', how='left')\n",
    "    \n",
    "    cols = ['COMID'] + [col for col in df_qc.columns if col != 'COMID']\n",
    "    df_qc = df_qc[cols]\n",
    "    df_qc.to_csv(f'2.swot-points-selection_iqr_{a}_{b}_{c}_{d}.csv', index=False)\n",
    "    \n",
    "    # 步骤4: 拟合\n",
    "    print(\"Step 4: Fitting hydraulic curves...\")\n",
    "    # 根据a选项决定是否跳过宽度筛选\n",
    "    skip_width_filter = (a == 'datemean')\n",
    "    fitter = HydraulicCurveFitter(df_w_stats, df_attrs, skip_width_filter=skip_width_filter)\n",
    "    df_fit_all = fitter.fit_all_stations(df_qc)\n",
    "    \n",
    "    if df_fit_all is None or len(df_fit_all) == 0:\n",
    "        print(f\"No fit data for {a}_{b}_{c}_{d}\")\n",
    "        return\n",
    "    \n",
    "    df_fit_all.to_csv(f'3.fit_proba_modified_q50_iqr_{a}_{b}_{c}_{d}.csv', index=False)\n",
    "    \n",
    "    # 步骤5: 生成曲线\n",
    "    print(\"Step 5: Generating hypsometric curves...\")\n",
    "    df_hypso = HypsometricCurveGenerator.generate_curves(df_fit_all)\n",
    "    \n",
    "    if df_hypso is None or len(df_hypso) == 0:\n",
    "        print(f\"No hypsometric curves generated for {a}_{b}_{c}_{d}\")\n",
    "        return\n",
    "    \n",
    "    df_hypso.to_csv(f'4.hypso_med_modified_q50_iqr_{a}_{b}_{c}_{d}.csv', index=False)\n",
    "    \n",
    "    # 步骤6: 验证（包含不确定性传播）\n",
    "    print(\"Step 6: Validating model with uncertainty propagation...\")\n",
    "    validator = ModelValidator()\n",
    "    df_width = common_data['df_width']\n",
    "    \n",
    "    # 【修改】传入 skip_width_filter 参数\n",
    "    df_results = validator.validate(\n",
    "        df_hypso, df_width,\n",
    "        '/home/xj/device5/data/daily_Q',\n",
    "        df_fit_all,\n",
    "        skip_width_filter=skip_width_filter  # 传递参数\n",
    "    )\n",
    "    \n",
    "    if df_results is None or len(df_results) == 0:\n",
    "        print(f\"No validation results for {a}_{b}_{c}_{d}\")\n",
    "        return\n",
    "    \n",
    "    df_results.to_csv(f'5.q_kge_med_modified_q50_iqr_{a}_{b}_{c}_{d}.csv', index=False)\n",
    "    \n",
    "    # ================================================================\n",
    "    # 【新增】步骤7: 生成不确定性分析报告和可视化\n",
    "    # ================================================================\n",
    "    print(\"Step 7: Generating uncertainty analysis report...\")\n",
    "    visualizer = UncertaintyVisualizer()\n",
    "    \n",
    "    # 覆盖率汇总图\n",
    "    visualizer.plot_coverage_summary(\n",
    "        df_results, \n",
    "        f'6.coverage_summary_{a}_{b}_{c}_{d}.png'\n",
    "    )\n",
    "    \n",
    "    # 覆盖率报告\n",
    "    station_summary, report = visualizer.create_coverage_report(\n",
    "        df_results,\n",
    "        f'6.coverage_report_{a}_{b}_{c}_{d}.csv'\n",
    "    )\n",
    "    \n",
    "    # 【新增】为前5个站点绘制时序图示例\n",
    "    sample_stations = df_results['stationid'].unique()[:5]\n",
    "    os.makedirs(f'timeseries_{a}_{b}_{c}_{d}', exist_ok=True)\n",
    "    for s in sample_stations:\n",
    "        df_station = df_results[df_results['stationid'] == s]\n",
    "        visualizer.plot_station_timeseries(\n",
    "            df_station, s,\n",
    "            f'timeseries_{a}_{b}_{c}_{d}/station_{s}.png'\n",
    "        )\n",
    "    \n",
    "    print(f\"Configuration {a}_{b}_{c}_{d} completed!\")\n",
    "    gc.collect()\n",
    "\n",
    "# ============================================================================\n",
    "# 配置生成函数\n",
    "# ============================================================================\n",
    "def generate_configs():\n",
    "    \"\"\"\n",
    "    生成所有有效的配置组合\n",
    "    规则: noqa只和node组合，qaloose和qastrict可以和所有A选项组合\n",
    "    \n",
    "    B选项现在是IQR倍数: '1.0', '1.5', '2.0', '2.5', '3.0', '4.0'\n",
    "    注意: \n",
    "    - 当a='node'时，遍历所有b选项，正常进行宽度筛选\n",
    "    - 当a='datemean'时，也遍历所有b选项（用于验证阶段），但拟合阶段不做宽度筛选\n",
    "    \"\"\"\n",
    "    A_options = ['node']\n",
    "    B_options = ['1.5']  # IQR倍数\n",
    "    C_options = ['noqa']\n",
    "    D_options = ['VersionD']\n",
    "    \n",
    "    configs = []\n",
    "    \n",
    "    for a in A_options:\n",
    "        for b in B_options:\n",
    "            for c in C_options:\n",
    "                for d in D_options:\n",
    "                    # noqa只和node组合\n",
    "                    if c == 'noqa' and a != 'node':\n",
    "                        continue\n",
    "                    configs.append((a, b, c, d))\n",
    "    \n",
    "    return configs\n",
    "\n",
    "# ============================================================================\n",
    "# 主程序\n",
    "# ============================================================================\n",
    "def main():\n",
    "    \"\"\"主程序流程\"\"\"\n",
    "    import time\n",
    "    total_start = time.time()\n",
    "    \n",
    "    # 【新增】打印不确定性分析配置\n",
    "    print(\"=\"*60)\n",
    "    print(\"WIDTH UNCERTAINTY PROPAGATION ANALYSIS\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Pixel resolution: {PIXEL_RESOLUTION} m\")\n",
    "    print(f\"Width uncertainty: ±{2*WIDTH_JITTER_PER_BANK} m (±1 pixel per bank)\")\n",
    "    print(f\"Monte Carlo samples: {N_MC_SAMPLES}\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    print(\"\\nLoading common data...\")\n",
    "    df_l8 = pd.read_csv('../2-preprocess/1.gages3000_glow_datemean_width_timeseries.csv')\n",
    "    df_comid = pd.read_csv('../2-preprocess/4.q50_weighted_slp.csv')[['stationid', 'COMID']]\n",
    "    df_attrs = pd.read_csv('../2-preprocess/4.q50_weighted_slp.csv')\n",
    "    df_width = pd.read_csv('../2-preprocess/1.gages3000_glow_datemean_width_timeseries.csv')\n",
    "    df_width['date'] = pd.to_datetime(df_width['date'])\n",
    "    \n",
    "    common_data = {\n",
    "        'df_l8': df_l8,\n",
    "        'df_comid': df_comid,\n",
    "        'df_attrs': df_attrs,\n",
    "        'df_width': df_width\n",
    "    }\n",
    "    \n",
    "    # 生成有效配置\n",
    "    configs = generate_configs()\n",
    "    \n",
    "    print(f\"\\nTotal configurations to run: {len(configs)}\")\n",
    "    print(\"Configurations:\")\n",
    "    for cfg in configs:\n",
    "        print(f\"  {cfg}\")\n",
    "    \n",
    "    # 运行所有配置\n",
    "    for a, b, c, d in configs:\n",
    "        start = time.time()\n",
    "        run_configuration(a, b, c, d, common_data)\n",
    "        print(f\"Time for ({a}, {b}, {c}, {d}): {time.time() - start:.2f}s\")\n",
    "    \n",
    "    # 生成箱型图（包含新指标）\n",
    "    print(\"\\nGenerating boxplot comparisons...\")\n",
    "    metrics = ['kge', 'nse', 'nrmse', 'coverage_95', 'coverage_50']  # 【新增】覆盖率指标\n",
    "    data_dict = {metric: [] for metric in metrics}\n",
    "    labels = []\n",
    "    \n",
    "    for a, b, c, d in configs:\n",
    "        file = f'5.q_kge_med_modified_q50_iqr_{a}_{b}_{c}_{d}.csv'\n",
    "        if os.path.exists(file):\n",
    "            df = pd.read_csv(file)\n",
    "            label = f'{a}_{b}_{c}_{d}'\n",
    "            labels.append(label)\n",
    "            for metric in metrics:\n",
    "                if metric in df.columns:\n",
    "                    station_metrics = df.groupby('stationid')[metric].first().values\n",
    "                    data_dict[metric].append(station_metrics)\n",
    "        \n",
    "    for metric in metrics:\n",
    "        if data_dict[metric]:\n",
    "            fig, ax = plt.subplots(figsize=(14, 6))\n",
    "            ax.boxplot(data_dict[metric], labels=labels)\n",
    "            \n",
    "            # 【新增】添加期望线（对于覆盖率）\n",
    "            if metric == 'coverage_95':\n",
    "                ax.axhline(y=0.95, color='red', linestyle='--', label='Expected (95%)')\n",
    "                ax.legend()\n",
    "            elif metric == 'coverage_50':\n",
    "                ax.axhline(y=0.50, color='red', linestyle='--', label='Expected (50%)')\n",
    "                ax.legend()\n",
    "            \n",
    "            ax.set_title(f'{metric.upper()} Boxplot Comparison')\n",
    "            ax.set_xlabel('Configuration (A_B_C_D)')\n",
    "            ax.set_ylabel(metric.upper())\n",
    "            plt.xticks(rotation=45, ha='right')\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(f'boxplot_{metric}.png', dpi=150)\n",
    "            plt.close()\n",
    "    \n",
    "    # 【新增】生成汇总对比表\n",
    "    print(\"\\nGenerating summary comparison table...\")\n",
    "    summary_data = []\n",
    "    for a, b, c, d in configs:\n",
    "        report_file = f'6.coverage_report_{a}_{b}_{c}_{d}_summary.csv'\n",
    "        if os.path.exists(report_file):\n",
    "            report = pd.read_csv(report_file)\n",
    "            report['config_a'] = a\n",
    "            report['config_b'] = b\n",
    "            report['config_c'] = c\n",
    "            report['config_d'] = d\n",
    "            summary_data.append(report)\n",
    "    \n",
    "    if summary_data:\n",
    "        summary_df = pd.concat(summary_data, ignore_index=True)\n",
    "        summary_df.to_csv('7.all_configs_summary.csv', index=False)\n",
    "        print(\"\\nAll configurations summary saved to: 7.all_configs_summary.csv\")\n",
    "    \n",
    "    print(f\"\\nTotal time: {time.time() - total_start:.2f}s\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
