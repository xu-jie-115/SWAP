{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "adc5434a-0f48-402d-a109-7421db41a647",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "station Brazil_10800000 length < 10. Skipping...\n",
      "station Brazil_56846080 length < 10. Skipping...\n",
      "station Brazil_83461000 length < 10. Skipping...\n",
      "station EWA_9114227 length < 10. Skipping...\n",
      "station GRDC_1112100 length < 10. Skipping...\n",
      "station GRDC_1134200 length < 10. Skipping...\n",
      "station GRDC_1134400 length < 10. Skipping...\n",
      "station GRDC_1134450 length < 10. Skipping...\n",
      "station GRDC_1134460 length < 10. Skipping...\n",
      "station GRDC_1496500 length < 10. Skipping...\n",
      "station GRDC_1531600 length < 10. Skipping...\n",
      "station GRDC_1537100 length < 10. Skipping...\n",
      "station GRDC_1737150 length < 10. Skipping...\n",
      "station GRDC_1748500 length < 10. Skipping...\n",
      "station GRDC_1749500 length < 10. Skipping...\n",
      "station GRDC_1835900 length < 10. Skipping...\n",
      "station GRDC_2369905 length < 10. Skipping...\n",
      "station GRDC_2469112 length < 10. Skipping...\n",
      "station GRDC_2469140 length < 10. Skipping...\n",
      "station GRDC_2587220 length < 10. Skipping...\n",
      "station GRDC_2906880 length < 10. Skipping...\n",
      "station GRDC_2910606 length < 10. Skipping...\n",
      "station GRDC_2969430 length < 10. Skipping...\n",
      "station GRDC_3106300 length < 10. Skipping...\n",
      "station GRDC_3469150 length < 10. Skipping...\n",
      "station GRDC_4102700 length < 10. Skipping...\n",
      "station GRDC_4102740 length < 10. Skipping...\n",
      "station GRDC_4103750 length < 10. Skipping...\n",
      "station GRDC_4203503 length < 10. Skipping...\n",
      "station GRDC_4214940 length < 10. Skipping...\n",
      "station GRDC_4356600 length < 10. Skipping...\n",
      "station GRDC_4356700 length < 10. Skipping...\n",
      "station GRDC_6348400 length < 10. Skipping...\n",
      "station GRDC_6348500 length < 10. Skipping...\n",
      "station GRDC_6348800 length < 10. Skipping...\n",
      "station GRDC_6981800 length < 10. Skipping...\n",
      "station USGS_15291000 length < 10. Skipping...\n",
      "station USGS_15291200 length < 10. Skipping...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from scipy.stats import spearmanr\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "DRAW_FIGURE = False\n",
    "\n",
    "# NSE (Nash-Sutcliffe Efficiency)\n",
    "def nse(observed, simulated):\n",
    "    return 1 - (np.sum((observed - simulated) ** 2) / np.sum((observed - np.mean(observed)) ** 2))\n",
    "\n",
    "# KGE (Kling-Gupta Efficiency)\n",
    "def kge(observed, simulated):\n",
    "    r = np.corrcoef(observed, simulated)[0, 1]\n",
    "    alpha = np.mean(simulated) / np.mean(observed)\n",
    "    beta  = np.std(simulated)/np.mean(simulated) / (np.std(observed)/np.mean(observed))\n",
    "    return 1 - np.sqrt((r - 1) ** 2 + (alpha - 1) ** 2 + (beta - 1) ** 2)\n",
    "\n",
    "# Relative RMSE\n",
    "def relative_rmse(observed, simulated):\n",
    "    rmse = np.sqrt(mean_squared_error(observed, simulated))\n",
    "    return rmse / np.mean(observed)\n",
    "\n",
    "def fit_function(w,  z0, u1, s1):\n",
    "    return z0 + u1 * (w ** s1)\n",
    "\n",
    "df_fit_all = pd.read_csv('3/fit_proba_modified_q50.csv')\n",
    "df_med_all = pd.read_csv('3/hypso_med_modified_q50.csv')\n",
    "\n",
    "\n",
    "folder_path = 'daily_long/daily_long'\n",
    "csv_files = [f for f in os.listdir(folder_path) if f.endswith('.csv')]\n",
    "\n",
    "\n",
    "width_df = pd.read_csv('gages3000_glow_datemean_width_timeseries.csv')\n",
    "# df_adcp = pds.merge(width_df,on=['stationid','time'],how = 'inner')\n",
    "width_df['date'] = pd.to_datetime(width_df['date'], errors='coerce')  # Similarly for width_df\n",
    "\n",
    "stationids = sorted(df_fit_all['stationid'].unique())\n",
    "\n",
    "# 创建一个空的DataFrame用于存储所有数据\n",
    "pds = pd.DataFrame()\n",
    "\n",
    "# 从1979-01-01开始，逐日增加\n",
    "start_date = pd.to_datetime('1979-01-01')\n",
    "\n",
    "df_res = []\n",
    "co, co_ori = 0, 0\n",
    "\n",
    "for s in stationids:\n",
    "    file_path = os.path.join(folder_path, s+'.csv')\n",
    "    if not os.path.exists(file_path):\n",
    "        print(f\"File {file_path} does not exist. Skipping...\")\n",
    "        continue\n",
    "\n",
    "    \n",
    "    df_val = pd.read_csv(file_path)\n",
    "    # 生成逐日增加的时间列\n",
    "    num_days = len(df_val)\n",
    "    time_range = pd.date_range(start=start_date, periods=num_days, freq='D')\n",
    "    df_val['date'] = time_range\n",
    "    df_val['stationid'] = s\n",
    "    df_val = df_val.dropna(subset=['qobs'])\n",
    "    df_val = df_val.merge(width_df[width_df['stationid']==s],on=['stationid','date'],how = 'inner')\n",
    "\n",
    "    df_fit = df_fit_all[df_fit_all['stationid']==s]\n",
    "    df_med = df_med_all[df_med_all['stationid']==s].reset_index(drop=True)\n",
    "    \n",
    "    w_low, w_high, slp = df_fit.iloc[0][['w_low','w_high','slp']]\n",
    "\n",
    "    df_val_ori = df_val.drop_duplicates('date')\n",
    "    df_val = df_val[(df_val['width']>=w_low) & (df_val['width']<=w_high)]\n",
    "    df_val = df_val.drop_duplicates('date')\n",
    "    co_ori += len(df_val_ori)\n",
    "    co += len(df_val)\n",
    "    \n",
    "    # 向量化操作代替 apply\n",
    "    idx_w = np.searchsorted(df_med['width'].values, df_val['width'].values)\n",
    "    df_val['idx_w'] = idx_w\n",
    "    df_val['width_i-1'] = df_med['width'].iloc[idx_w - 1].values\n",
    "    df_val['width_i'] = df_med['width'].iloc[idx_w].values\n",
    "    df_val['wse_i-1'] = df_med['wse'].iloc[idx_w - 1].values\n",
    "    df_val['wse_i'] = df_med['wse'].iloc[idx_w].values\n",
    "    df_val['area_i-1'] = df_med['area'].iloc[idx_w - 1].values\n",
    "\n",
    "    # 使用向量化计算 area_hypso\n",
    "    width_diff = df_val['width'] - df_val['width_i-1']\n",
    "    width_diff_i = df_val['width_i'] - df_val['width_i-1']\n",
    "    area_hypso = df_val['area_i-1'] + 0.5 * (df_val['width_i-1'] + df_val['width']) * \\\n",
    "                 (df_val['wse_i'] - df_val['wse_i-1']) * width_diff * (1 / width_diff_i)\n",
    "    df_val['area_hypso'] = area_hypso\n",
    "\n",
    "    # 使用向量化计算 Q_est\n",
    "    df_val['Q_est'] = df_val['area_hypso'] ** (5.0 / 3.0) * df_val['width'] ** (-2.0 / 3.0) * \\\n",
    "                      slp ** 0.5 / 0.035\n",
    "    df_val = df_val.dropna()\n",
    "    if len(df_val)<10:\n",
    "        print(f\"station {s} length < 10. Skipping...\")\n",
    "        continue\n",
    "        \n",
    "\n",
    "    df_val['kge'] = kge(df_val['qobs'], df_val['Q_est'])\n",
    "    df_val['nse'] = nse(df_val['qobs'], df_val['Q_est'])\n",
    "    df_val['nrmse'] = relative_rmse(df_val['qobs'], df_val['Q_est'])\n",
    "    df_res.append(df_val[['stationid','date','width','area_hypso','qobs','Q_est','kge','nse','nrmse']])\n",
    "\n",
    "\n",
    "#print(co, co_ori, co/co_ori)\n",
    "df_res = pd.concat(df_res, ignore_index=True)\n",
    "df_res.to_csv('3/q_kge_med_modified_q50.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
