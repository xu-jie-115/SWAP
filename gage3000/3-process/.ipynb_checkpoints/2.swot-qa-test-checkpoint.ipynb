{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b5dc27c7-8cfc-487a-a6cd-15b430e456e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['node_id', 'reach_id', 'time', 'lat', 'lon', 'wse', 'wse_u', 'width',\n",
      "       'width_u', 'node_q_b', 'node_q', 'ice_clim_f', 'dark_frac', 'p_width',\n",
      "       'p_n_ch_max', 'p_n_ch_mod', 'xovr_cal_q', 'xtrk_dist', 'wse_r_u',\n",
      "       'p_length', 'file_name'],\n",
      "      dtype='object')\n",
      "          node_id     reach_id          time           lat           lon  \\\n",
      "0  22160300290761  22160300291  7.925395e+08 -1.000000e+12 -1.000000e+12   \n",
      "1  22160300290771  22160300291  7.925395e+08 -1.000000e+12 -1.000000e+12   \n",
      "\n",
      "            wse         wse_u      width   width_u  node_q_b  node_q  \\\n",
      "0 -1.000000e+12 -1.000000e+12   2.135025  0.004025  58723843       3   \n",
      "1 -1.000000e+12 -1.000000e+12  78.151521  0.228697  58722307       3   \n",
      "\n",
      "   ice_clim_f  dark_frac  p_width  p_n_ch_max  p_n_ch_mod  xovr_cal_q  \\\n",
      "0           1        1.0     63.0           1           1           2   \n",
      "1           1        1.0     84.0           1           1           2   \n",
      "\n",
      "     xtrk_dist       wse_r_u    p_length  \\\n",
      "0  59642.86719 -1.000000e+12  201.890141   \n",
      "1  59640.81250 -1.000000e+12  231.661762   \n",
      "\n",
      "                                                                        file_name  \n",
      "0  SWOT_L2_HR_RiverSP_Node_028_206_EU_20250210T214232_20250210T215317_PIC2_01.zip  \n",
      "1  SWOT_L2_HR_RiverSP_Node_028_206_EU_20250210T214232_20250210T215317_PIC2_01.zip  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import glob\n",
    "import os\n",
    "\n",
    "pd.set_option('display.max_columns', None)      # 显示所有列\n",
    "pd.set_option('display.max_rows', None)         # 显示所有行\n",
    "pd.set_option('display.width', None)            # 不限制显示宽度\n",
    "pd.set_option('display.max_colwidth', None)     # 不限制列宽\n",
    "\n",
    "csv_path = '/shared5/RESEARCH_DATA/SWOT/VersionC/processed/node_20250210.csv'\n",
    "df = pd.read_csv(csv_path)\n",
    "print(df.columns)\n",
    "print(df.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "82302f80-eedf-4e5d-a068-b23bc13d3cea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "初始数据量: 127625\n",
      "过滤后: 58947\n",
      "严格过滤后: 27785\n",
      "\n",
      "=== 统计摘要 ===\n",
      "总站点数: 1938\n",
      "完全被过滤的站点数: 453\n",
      "平均保留率: 19.90%\n",
      "\n",
      "保留数据最多的前10个站点:\n",
      "          stationid  count_before  count_after  retention_rate(%)\n",
      "0     USGS_11377100           177          139              78.53\n",
      "1      GRDC_6935500           132          110              83.33\n",
      "2      GRDC_4207320           115           93              80.87\n",
      "3     USGS_01011000           124           86              69.35\n",
      "4     USGS_06192500           118           85              72.03\n",
      "5     USGS_12414500           125           85              68.00\n",
      "6      GRDC_4206250           122           84              68.85\n",
      "7     USGS_05443500           143           84              58.74\n",
      "8      GRDC_4206920           120           83              69.17\n",
      "9      GRDC_5654140           104           78              75.00\n",
      "10  Brazil_20489100           115           77              66.96\n",
      "\n",
      "过滤后的数据已保存到: ./2.swot_qa.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "data_v = 'VersionD'\n",
    "model = 'strict'\n",
    "\n",
    "# 读取数据\n",
    "df_original = pd.read_csv(f'./1.all_matched_points_{data_v}.csv')\n",
    "\n",
    "# 统计过滤前每个stationid的数据量\n",
    "before_counts = df_original.groupby('stationid').size().reset_index(name='count_before')\n",
    "\n",
    "# 开始过滤\n",
    "df = df_original.copy()\n",
    "print(f\"初始数据量: {len(df)}\")\n",
    "\n",
    "# 替换缺失值并删除\n",
    "df = df.replace(-999999999999, np.nan).dropna()\n",
    "# print(f\"删除缺失值后: {len(df)}\")\n",
    "\n",
    "# 应用各种过滤条件\n",
    "'''bad node_q_b quality'''\n",
    "df = df[df['node_q_b'] <= 2194304]   \n",
    "# print(f\"过滤 node_q_b <= 2194304 后: {len(df)}\")\n",
    "\n",
    "df = df[df['node_q'] <= 1]\n",
    "# print(f\"过滤 node_q <= 1 后: {len(df)}\")\n",
    "\n",
    "df = df[df['dark_frac'] <= 0.5]\n",
    "# print(f\"过滤 dark_frac <= 0.4 后: {len(df)}\")\n",
    "\n",
    "'''Quality of the cross-over calibration, Height correction from KaRIn crossover calibration.'''\n",
    "df = df[df['xovr_cal_q'] <= 1] \n",
    "# print(f\"过滤 xovr_cal_q <= 1 后: {len(df)}\")\n",
    "\n",
    "df = df[(df['xtrk_dist'].abs() > 15000) & (df['xtrk_dist'].abs() < 60000)]\n",
    "# print(f\"过滤 15 < |xtrk_dist| < 60 后: {len(df)}\")\n",
    "\n",
    "df = df[df['ice_clim_f'] <= 1]\n",
    "# print(f\"过滤 ice_clim_f <= 1 后: {len(df)}\")\n",
    "print(f\"过滤后: {len(df)}\")\n",
    "\n",
    "if model == 'strict':\n",
    "    ''' classification_qual_suspect '''\n",
    "    df = df[df['node_q_b'] & (1 << 1) == 0]\n",
    "    # print(f\"过滤 node_q_b bit 1 后: {len(df)}\")\n",
    "    \n",
    "    ''' geolocation_qual_suspect '''\n",
    "    df = df[df['node_q_b'] & (1 << 2) == 0]\n",
    "    # print(f\"过滤 node_q_b bit 2 后: {len(df)}\")\n",
    "    \n",
    "    ''' water_fraction_suspect '''\n",
    "    df = df[df['node_q_b'] & (1 << 3) == 0]\n",
    "    # print(f\"过滤 node_q_b bit 3 后: {len(df)}\")\n",
    "    \n",
    "    ''' few_wse_observations '''\n",
    "    df = df[df['node_q_b'] & (1 << 11) == 0]\n",
    "    # print(f\"过滤 node_q_b bit 11 后: {len(df)}\")\n",
    "    \n",
    "    ''' geolocation_qual_degraded '''\n",
    "    df = df[df['node_q_b'] & (1 << 19) == 0]\n",
    "    # print(f\"过滤 node_q_b bit 19 后: {len(df)}\")\n",
    "    \n",
    "    df = df[df['p_width'] >= 30.0]\n",
    "    # print(f\"过滤 p_width >= 30.0 后: {len(df)}\")\n",
    "    \n",
    "    df = df[df['wse_r_u'] < 0.5]\n",
    "    # print(f\"过滤 wse_r_u < 0.5 后: {len(df)}\")\n",
    "    \n",
    "    # df = df[abs(df['p_length']) > 7]\n",
    "    # print(f\"过滤 |p_length| > 7 后: {len(df)}\")\n",
    "    print(f\"严格过滤后: {len(df)}\")\n",
    "\n",
    "df = df.drop(columns=['index_right'])\n",
    "df['date'] = pd.to_datetime('2000-01-01') + pd.to_timedelta(df['time'], unit='s')\n",
    "df['date'] = df['date'].dt.date\n",
    "df = df.drop_duplicates()\n",
    "\n",
    "# print(f\"\\n最终数据量: {len(df)}\")\n",
    "# print(f\"总体保留率: {len(df)/len(df_original)*100:.2f}%\")\n",
    "\n",
    "# 统计过滤后每个stationid的数据量\n",
    "after_counts = df.groupby('stationid').size().reset_index(name='count_after')\n",
    "\n",
    "# 合并前后统计结果\n",
    "comparison = before_counts.merge(after_counts, on='stationid', how='left')\n",
    "comparison['count_after'] = comparison['count_after'].fillna(0).astype(int)\n",
    "\n",
    "# 计算变化\n",
    "comparison['retention_rate(%)'] = (comparison['count_after'] / comparison['count_before'] * 100).round(2)\n",
    "\n",
    "# 排序（按照保留数据量降序）\n",
    "comparison = comparison.sort_values('count_after', ascending=False)\n",
    "\n",
    "# 添加汇总行\n",
    "summary_row = pd.DataFrame({\n",
    "    'stationid': ['TOTAL'],\n",
    "    'count_before': [comparison['count_before'].sum()],\n",
    "    'count_after': [comparison['count_after'].sum()],\n",
    "    'retention_rate(%)': [(comparison['count_after'].sum() / comparison['count_before'].sum() * 100)]\n",
    "})\n",
    "comparison = pd.concat([comparison, summary_row], ignore_index=True)\n",
    "\n",
    "# # 保存对比结果\n",
    "# output_file = './2.swot_qa_comparison.csv'\n",
    "# comparison.to_csv(output_file, index=False)\n",
    "# print(f\"\\n对比结果已保存到: {output_file}\")\n",
    "\n",
    "# 显示统计摘要\n",
    "print(\"\\n=== 统计摘要 ===\")\n",
    "print(f\"总站点数: {len(comparison)-1}\")  # 减去汇总行\n",
    "print(f\"完全被过滤的站点数: {(comparison['count_after'] == 0).sum() - 1}\")  # 减去汇总行\n",
    "print(f\"平均保留率: {comparison[comparison['stationid'] != 'TOTAL']['retention_rate(%)'].mean():.2f}%\")\n",
    "print(f\"\\n保留数据最多的前10个站点:\")\n",
    "print(comparison.head(11)[['stationid', 'count_before', 'count_after', 'retention_rate(%)']])\n",
    "\n",
    "# 保存过滤后的数据\n",
    "df.to_csv(f'./2.swot_qa_node_qa{model}_{data_v}.csv', index=False)\n",
    "print(f\"\\n过滤后的数据已保存到: ./2.swot_qa.csv\")\n",
    "\n",
    "result = df.groupby(['date', 'stationid','node_id','reach_id','COMID'])[['wse', 'wse_u', 'width','width_u']].mean().reset_index()\n",
    "result.to_csv(f'2.swot_qa_datemean_qa{model}_{data_v}.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
