{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7d66702b-d03b-4e9c-ac59-93d52f5f41fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "未找到5.*.csv文件,请确保文件在当前目录下\n",
      "找到 0 个文件: []\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "No objects to concatenate",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 41\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m -> \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlabel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(df)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m 行, NaN数量: kge=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdf[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mkge\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39misna()\u001b[38;5;241m.\u001b[39msum()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, nrmse=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdf[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnrmse\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39misna()\u001b[38;5;241m.\u001b[39msum()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, nse=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdf[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnse\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39misna()\u001b[38;5;241m.\u001b[39msum()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m# 合并所有数据\u001b[39;00m\n\u001b[0;32m---> 41\u001b[0m combined_df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mall_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;66;03m# 替换inf值为NaN以便统计\u001b[39;00m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mkge\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnrmse\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnse\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n",
      "File \u001b[0;32m~/anaconda3/envs/python39/lib/python3.9/site-packages/pandas/core/reshape/concat.py:382\u001b[0m, in \u001b[0;36mconcat\u001b[0;34m(objs, axis, join, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[0m\n\u001b[1;32m    379\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m copy \u001b[38;5;129;01mand\u001b[39;00m using_copy_on_write():\n\u001b[1;32m    380\u001b[0m     copy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m--> 382\u001b[0m op \u001b[38;5;241m=\u001b[39m \u001b[43m_Concatenator\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    383\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobjs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    384\u001b[0m \u001b[43m    \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    385\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    386\u001b[0m \u001b[43m    \u001b[49m\u001b[43mjoin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    387\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkeys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    388\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlevels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    389\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnames\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnames\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    390\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverify_integrity\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverify_integrity\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    391\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    392\u001b[0m \u001b[43m    \u001b[49m\u001b[43msort\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    393\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    395\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m op\u001b[38;5;241m.\u001b[39mget_result()\n",
      "File \u001b[0;32m~/anaconda3/envs/python39/lib/python3.9/site-packages/pandas/core/reshape/concat.py:445\u001b[0m, in \u001b[0;36m_Concatenator.__init__\u001b[0;34m(self, objs, axis, join, keys, levels, names, ignore_index, verify_integrity, copy, sort)\u001b[0m\n\u001b[1;32m    442\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverify_integrity \u001b[38;5;241m=\u001b[39m verify_integrity\n\u001b[1;32m    443\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy \u001b[38;5;241m=\u001b[39m copy\n\u001b[0;32m--> 445\u001b[0m objs, keys \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_clean_keys_and_objs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobjs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    447\u001b[0m \u001b[38;5;66;03m# figure out what our result ndim is going to be\u001b[39;00m\n\u001b[1;32m    448\u001b[0m ndims \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_ndims(objs)\n",
      "File \u001b[0;32m~/anaconda3/envs/python39/lib/python3.9/site-packages/pandas/core/reshape/concat.py:507\u001b[0m, in \u001b[0;36m_Concatenator._clean_keys_and_objs\u001b[0;34m(self, objs, keys)\u001b[0m\n\u001b[1;32m    504\u001b[0m     objs_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(objs)\n\u001b[1;32m    506\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(objs_list) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 507\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo objects to concatenate\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    509\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m keys \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    510\u001b[0m     objs_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(com\u001b[38;5;241m.\u001b[39mnot_none(\u001b[38;5;241m*\u001b[39mobjs_list))\n",
      "\u001b[0;31mValueError\u001b[0m: No objects to concatenate"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import os\n",
    "\n",
    "# 读取所有5.*.csv文件\n",
    "csv_files = glob.glob('5.*qaloose.csv')\n",
    "if not csv_files:\n",
    "    print(\"未找到5.*.csv文件,请确保文件在当前目录下\")\n",
    "    exit()\n",
    "\n",
    "print(f\"找到 {len(csv_files)} 个文件: {csv_files}\")\n",
    "\n",
    "# 存储所有数据\n",
    "all_data = []\n",
    "source_counts = {}  # 存储每个来源的数据量\n",
    "\n",
    "for file in csv_files:\n",
    "    df = pd.read_csv(file)\n",
    "    # 选择需要的列\n",
    "    df = df[['stationid', 'kge', 'nrmse', 'nse']]\n",
    "    # 丢掉重复行\n",
    "    df = df.drop_duplicates()\n",
    "    \n",
    "    # 确保数值列为数值类型\n",
    "    for col in ['kge', 'nrmse', 'nse']:\n",
    "        df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "    \n",
    "    # 获取文件名(不含扩展名),只保留最后两位信息(如ori_15, smooth_10)\n",
    "    filename = os.path.splitext(file)[0]\n",
    "    parts = filename.split('_')\n",
    "    label = '_'.join(parts[-3:])\n",
    "    df['source'] = label\n",
    "    source_counts[label] = len(df)  # 记录数据量\n",
    "    all_data.append(df)\n",
    "    \n",
    "    # 打印更多调试信息\n",
    "    print(f\"{file} -> {label}: {len(df)} 行, NaN数量: kge={df['kge'].isna().sum()}, nrmse={df['nrmse'].isna().sum()}, nse={df['nse'].isna().sum()}\")\n",
    "\n",
    "# 合并所有数据\n",
    "combined_df = pd.concat(all_data, ignore_index=True)\n",
    "\n",
    "# 替换inf值为NaN以便统计\n",
    "for col in ['kge', 'nrmse', 'nse']:\n",
    "    combined_df[col] = combined_df[col].replace([float('inf'), float('-inf')], float('nan'))\n",
    "\n",
    "# 获取所有文件来源\n",
    "sources = combined_df['source'].unique()\n",
    "print(f\"\\n数据源: {list(sources)}\")\n",
    "\n",
    "# 准备箱型图数据\n",
    "metrics = ['kge', 'nrmse', 'nse']\n",
    "y_limits = {\n",
    "    'kge': (-2, 1),\n",
    "    'nrmse': (0, 3),\n",
    "    'nse': (-6, 1)\n",
    "}\n",
    "\n",
    "# ============== 新增：计算并打印统计信息 ==============\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"各来源各指标的统计信息（中值 Median 和 平均值 Mean）\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# 方法1：使用groupby一次性计算\n",
    "stats_df = combined_df.groupby('source')[metrics].agg(['median', 'mean', 'std', 'count'])\n",
    "print(\"\\n【详细统计表】\")\n",
    "print(stats_df.round(4).to_string())\n",
    "\n",
    "# 方法2：更美观的格式化输出\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"【格式化统计摘要】\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# 定义各指标的阈值条件\n",
    "thresholds = {\n",
    "    'kge': ('>', 0),      # KGE > 0\n",
    "    'nrmse': ('<', 0.6),  # NRMSE < 0.6\n",
    "    'nse': ('>', 0)       # NSE > 0\n",
    "}\n",
    "\n",
    "# ============== 新增：计算达标百分比 ==============\n",
    "def calc_pass_rate(data, metric):\n",
    "    \"\"\"计算达标百分比\"\"\"\n",
    "    op, threshold = thresholds[metric]\n",
    "    valid_data = data.dropna()\n",
    "    if len(valid_data) == 0:\n",
    "        return 0.0\n",
    "    if op == '>':\n",
    "        pass_count = (valid_data > threshold).sum()\n",
    "    else:  # '<'\n",
    "        pass_count = (valid_data < threshold).sum()\n",
    "    return (pass_count / len(valid_data)) * 100\n",
    "\n",
    "\n",
    "for metric in metrics:\n",
    "\n",
    "    op, threshold = thresholds[metric]\n",
    "    print(f\"\\n>>> {metric.upper()} <<<\")\n",
    "    print(f\"{'来源':<25} {'中值(Median)':<15} {'平均值(Mean)':<15} {'标准差(Std)':<15} {'有效数量':<10}\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    for src in sources:\n",
    "        data = combined_df[combined_df['source'] == src][metric].dropna()\n",
    "        median_val = data.median()\n",
    "        mean_val = data.mean()\n",
    "        std_val = data.std()\n",
    "        count_val = len(data)\n",
    "        print(f\"{src:<25} {median_val:<15.4f} {mean_val:<15.4f} {std_val:<15.4f} {count_val:<10}\")\n",
    "\n",
    "    \n",
    "    print(f\"\\n>>> {metric.upper()} ({op} {threshold}) <<<\")\n",
    "    print(f\"{'来源':<25} {'达标数':<10} {'总数':<10} {'达标率(%)':<15}\")\n",
    "    print(\"-\" * 80)\n",
    "    for src in sources:\n",
    "        data = combined_df[combined_df['source'] == src][metric].dropna()\n",
    "        total = len(data)\n",
    "        if op == '>':\n",
    "            pass_count = (data > threshold).sum()\n",
    "        else:\n",
    "            pass_count = (data < threshold).sum()\n",
    "        pass_rate = (pass_count / total * 100) if total > 0 else 0\n",
    "        print(f\"{src:<25} {pass_count:<10} {total:<10} {pass_rate:<15.2f}\")\n",
    "\n",
    "\n",
    "\n",
    "# 方法3：创建汇总表格便于对比\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"【汇总对比表 - 中值】\")\n",
    "print(\"=\" * 80)\n",
    "median_summary = combined_df.groupby('source')[metrics].median()\n",
    "print(median_summary.round(4).to_string())\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"【汇总对比表 - 平均值】\")\n",
    "print(\"=\" * 80)\n",
    "mean_summary = combined_df.groupby('source')[metrics].mean()\n",
    "print(mean_summary.round(4).to_string())\n",
    "\n",
    "# ============== 绘图部分 ==============\n",
    "# 创建图形 - 三个子图\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 6))\n",
    "\n",
    "for idx, metric in enumerate(metrics):\n",
    "    ax = axes[idx]\n",
    "    \n",
    "    # 准备每个来源的数据(过滤NaN值)\n",
    "    data_to_plot = []\n",
    "    valid_counts = []\n",
    "    medians = []\n",
    "    means = []\n",
    "    \n",
    "    for src in sources:\n",
    "        data = combined_df[combined_df['source'] == src][metric].dropna().values\n",
    "        data_to_plot.append(data)\n",
    "        valid_counts.append(len(data))\n",
    "        medians.append(pd.Series(data).median())\n",
    "        means.append(pd.Series(data).mean())\n",
    "        print(f\"  {metric} - {src}: {len(data)} 个有效值, median={pd.Series(data).median():.4f}, mean={pd.Series(data).mean():.4f}\")\n",
    "    \n",
    "    # 绘制箱型图\n",
    "    bp = ax.boxplot(data_to_plot, labels=sources, patch_artist=True)\n",
    "    \n",
    "    # 设置颜色\n",
    "    colors = plt.cm.Set3.colors[:len(sources)]\n",
    "    for patch, color in zip(bp['boxes'], colors):\n",
    "        patch.set_facecolor(color)\n",
    "    \n",
    "    # 在每个箱子上方标注数据量 n=xxx 和中值\n",
    "    for i, (src, n, med, mn) in enumerate(zip(sources, valid_counts, medians, means)):\n",
    "        ax.text(i + 1, y_limits[metric][1], f'n={n}\\nmed={med:.3f}', \n",
    "                ha='center', va='bottom', fontsize=8, fontweight='bold')\n",
    "    \n",
    "    # 设置标题和标签\n",
    "    ax.set_title(metric.upper(), fontsize=14, fontweight='bold')\n",
    "    ax.set_ylabel(metric.upper())\n",
    "    ax.set_xlabel('Source File')\n",
    "    \n",
    "    # 设置y轴范围(稍微扩大上限以显示n值和中值)\n",
    "    y_min, y_max = y_limits[metric]\n",
    "    ax.set_ylim(y_min, y_max + (y_max - y_min) * 0.15)\n",
    "    \n",
    "    # 旋转x轴标签\n",
    "    ax.tick_params(axis='x', rotation=90)\n",
    "    \n",
    "    # 添加网格\n",
    "    ax.grid(True, linestyle='--', alpha=0.7)\n",
    "\n",
    "plt.suptitle('Comparison of KGE, NRMSE, NSE across Different Files', fontsize=16, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "\n",
    "# 保存图片\n",
    "output_file = '6.metrics_comparison_boxplot.png'\n",
    "plt.savefig(output_file, dpi=300, bbox_inches='tight')\n",
    "print(f\"\\n图片已保存为: {output_file}\")\n",
    "\n",
    "# ============== 新增：将统计结果保存到CSV ==============\n",
    "# 保存详细统计到CSV文件\n",
    "stats_output = '6.metrics_statistics.csv'\n",
    "stats_df.to_csv(stats_output)\n",
    "print(f\"统计数据已保存为: {stats_output}\")\n",
    "\n",
    "# 保存简洁的中值和平均值对比表\n",
    "summary_data = []\n",
    "for src in sources:\n",
    "    row = {'source': src}\n",
    "    for metric in metrics:\n",
    "        data = combined_df[combined_df['source'] == src][metric].dropna()\n",
    "        row[f'{metric}_median'] = data.median()\n",
    "        row[f'{metric}_mean'] = data.mean()\n",
    "        row[f'{metric}_std'] = data.std()\n",
    "        row[f'{metric}_count'] = len(data)\n",
    "    summary_data.append(row)\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "summary_output = '6.metrics_summary.csv'\n",
    "summary_df.to_csv(summary_output, index=False)\n",
    "print(f\"汇总数据已保存为: {summary_output}\")\n",
    "\n",
    "# ============== 保存达标率统计到CSV ==============\n",
    "summary_data = []\n",
    "for src in sources:\n",
    "    row = {'source': src}\n",
    "    for metric in metrics:\n",
    "        data = combined_df[combined_df['source'] == src][metric].dropna()\n",
    "        op, threshold = thresholds[metric]\n",
    "        total = len(data)\n",
    "        if op == '>':\n",
    "            pass_count = (data > threshold).sum()\n",
    "        else:\n",
    "            pass_count = (data < threshold).sum()\n",
    "        pass_rate = (pass_count / total * 100) if total > 0 else 0\n",
    "        \n",
    "        row[f'{metric}_total'] = total\n",
    "        row[f'{metric}_pass_count'] = pass_count\n",
    "        row[f'{metric}_pass_rate(%)'] = round(pass_rate, 2)\n",
    "        row[f'{metric}_median'] = data.median()\n",
    "        row[f'{metric}_mean'] = data.mean()\n",
    "    summary_data.append(row)\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "print(summary_df)\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48aafb40-6ed0-470b-9a26-c57c617d7dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('5.q_kge_med_modified_q50_node_5_qaloose.csv')[['stationid','kge','nse','nrmse']]\n",
    "df = df.drop_duplicates()\n",
    "df.to_csv('5.q_kge_med_modified_q50_node_5_qaloose_metrics.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d66868f4-1f4b-4f7a-a763-d12a905aaaaf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
