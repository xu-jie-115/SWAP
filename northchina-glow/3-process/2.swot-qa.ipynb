{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "026d64e9-ef74-4339-a25c-d1ad8e382950",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "初始数据量: 2195\n",
      "过滤后: 780\n",
      "\n",
      "=== 统计摘要 ===\n",
      "总站点数: 21\n",
      "完全被过滤的站点数: 0\n",
      "平均保留率: 34.80%\n",
      "\n",
      "保留数据最多的前10个站点:\n",
      "      stationid  count_before  count_after  retention_rate(%)\n",
      "0       tieling           164           83              50.61\n",
      "1       lianhua           105           75              71.43\n",
      "2        tonghe           116           53              45.69\n",
      "3   longmenzhen           120           52              43.33\n",
      "4     liujiatun           117           49              41.88\n",
      "5     shihuiyao           141           48              34.04\n",
      "6       jiamusi            81           47              58.02\n",
      "7   shuanghetun           118           43              36.44\n",
      "8       kuerbin           107           43              40.19\n",
      "9    heishiguan            92           41              44.57\n",
      "10      tonghua           112           38              33.93\n",
      "\n",
      "过滤后的数据已保存到: ./2.swot_node_qaloose_VersionD.csv\n",
      "初始数据量: 2195\n",
      "过滤后: 780\n",
      "严格过滤后: 418\n",
      "\n",
      "=== 统计摘要 ===\n",
      "总站点数: 21\n",
      "完全被过滤的站点数: 0\n",
      "平均保留率: 18.35%\n",
      "\n",
      "保留数据最多的前10个站点:\n",
      "        stationid  count_before  count_after  retention_rate(%)\n",
      "0         tieling           164           61              37.20\n",
      "1         lianhua           105           37              35.24\n",
      "2       shihuiyao           141           33              23.40\n",
      "3         kuerbin           107           33              30.84\n",
      "4     shuanghetun           118           30              25.42\n",
      "5      heishiguan            92           27              29.35\n",
      "6       liujiatun           117           21              17.95\n",
      "7         jiamusi            81           19              23.46\n",
      "8   changjiangtun           117           19              16.24\n",
      "9        humaqiao            85           19              22.35\n",
      "10        baimasi            62           18              29.03\n",
      "\n",
      "过滤后的数据已保存到: ./2.swot_node_qastrict_VersionD.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "data_vs = ['VersionD']\n",
    "models  = ['loose','strict']\n",
    "\n",
    "for data_v in data_vs:\n",
    "    for model in models:\n",
    "    \n",
    "        # 读取数据\n",
    "        df_original = pd.read_csv(f'./1.all_matched_points_{data_v}.csv')\n",
    "        \n",
    "        # 统计过滤前每个stationid的数据量\n",
    "        before_counts = df_original.groupby('stationid').size().reset_index(name='count_before')\n",
    "        \n",
    "        # 开始过滤\n",
    "        df = df_original.copy()\n",
    "        print(f\"初始数据量: {len(df)}\")\n",
    "        \n",
    "        # 替换缺失值并删除\n",
    "        df = df.replace(-999999999999, np.nan).dropna()\n",
    "        # print(f\"删除缺失值后: {len(df)}\")\n",
    "        \n",
    "        # 应用各种过滤条件\n",
    "        '''bad node_q_b quality'''\n",
    "        df = df[df['node_q_b'] <= 2194304]   \n",
    "        # print(f\"过滤 node_q_b <= 2194304 后: {len(df)}\")\n",
    "        \n",
    "        df = df[df['node_q'] <= 1]\n",
    "        # print(f\"过滤 node_q <= 1 后: {len(df)}\")\n",
    "        \n",
    "        df = df[df['dark_frac'] <= 0.5]\n",
    "        # print(f\"过滤 dark_frac <= 0.4 后: {len(df)}\")\n",
    "        \n",
    "        '''Quality of the cross-over calibration, Height correction from KaRIn crossover calibration.'''\n",
    "        df = df[df['xovr_cal_q'] <= 1] \n",
    "        # print(f\"过滤 xovr_cal_q <= 1 后: {len(df)}\")\n",
    "        \n",
    "        df = df[(df['xtrk_dist'].abs() > 15000) & (df['xtrk_dist'].abs() < 60000)]\n",
    "        # print(f\"过滤 15 < |xtrk_dist| < 60 后: {len(df)}\")\n",
    "        \n",
    "        df = df[df['ice_clim_f'] <= 1]\n",
    "        # print(f\"过滤 ice_clim_f <= 1 后: {len(df)}\")\n",
    "        print(f\"过滤后: {len(df)}\")\n",
    "        \n",
    "        if model == 'strict':\n",
    "            ''' classification_qual_suspect '''\n",
    "            df = df[df['node_q_b'] & (1 << 1) == 0]\n",
    "            # print(f\"过滤 node_q_b bit 1 后: {len(df)}\")\n",
    "            \n",
    "            ''' geolocation_qual_suspect '''\n",
    "            df = df[df['node_q_b'] & (1 << 2) == 0]\n",
    "            # print(f\"过滤 node_q_b bit 2 后: {len(df)}\")\n",
    "            \n",
    "            ''' water_fraction_suspect '''\n",
    "            df = df[df['node_q_b'] & (1 << 3) == 0]\n",
    "            # print(f\"过滤 node_q_b bit 3 后: {len(df)}\")\n",
    "            \n",
    "            ''' few_wse_observations '''\n",
    "            df = df[df['node_q_b'] & (1 << 11) == 0]\n",
    "            # print(f\"过滤 node_q_b bit 11 后: {len(df)}\")\n",
    "            \n",
    "            ''' geolocation_qual_degraded '''\n",
    "            df = df[df['node_q_b'] & (1 << 19) == 0]\n",
    "            # print(f\"过滤 node_q_b bit 19 后: {len(df)}\")\n",
    "            \n",
    "            df = df[df['p_width'] >= 30.0]\n",
    "            # print(f\"过滤 p_width >= 30.0 后: {len(df)}\")\n",
    "            \n",
    "            df = df[df['wse_r_u'] < 0.5]\n",
    "            # print(f\"过滤 wse_r_u < 0.5 后: {len(df)}\")\n",
    "            \n",
    "            # df = df[abs(df['p_length']) > 7]\n",
    "            # print(f\"过滤 |p_length| > 7 后: {len(df)}\")\n",
    "            print(f\"严格过滤后: {len(df)}\")\n",
    "        \n",
    "        df = df.drop(columns=['index_right'])\n",
    "        df['date'] = pd.to_datetime('2000-01-01') + pd.to_timedelta(df['time'], unit='s')\n",
    "        df['date'] = df['date'].dt.date\n",
    "        df = df.drop_duplicates()\n",
    "        \n",
    "        # print(f\"\\n最终数据量: {len(df)}\")\n",
    "        # print(f\"总体保留率: {len(df)/len(df_original)*100:.2f}%\")\n",
    "        \n",
    "        # 统计过滤后每个stationid的数据量\n",
    "        after_counts = df.groupby('stationid').size().reset_index(name='count_after')\n",
    "        \n",
    "        # 合并前后统计结果\n",
    "        comparison = before_counts.merge(after_counts, on='stationid', how='left')\n",
    "        comparison['count_after'] = comparison['count_after'].fillna(0).astype(int)\n",
    "        \n",
    "        # 计算变化\n",
    "        comparison['retention_rate(%)'] = (comparison['count_after'] / comparison['count_before'] * 100).round(2)\n",
    "        \n",
    "        # 排序（按照保留数据量降序）\n",
    "        comparison = comparison.sort_values('count_after', ascending=False)\n",
    "        \n",
    "        # 添加汇总行\n",
    "        summary_row = pd.DataFrame({\n",
    "            'stationid': ['TOTAL'],\n",
    "            'count_before': [comparison['count_before'].sum()],\n",
    "            'count_after': [comparison['count_after'].sum()],\n",
    "            'retention_rate(%)': [(comparison['count_after'].sum() / comparison['count_before'].sum() * 100)]\n",
    "        })\n",
    "        comparison = pd.concat([comparison, summary_row], ignore_index=True)\n",
    "        \n",
    "        # # 保存对比结果\n",
    "        # output_file = './2.swot_qa_comparison.csv'\n",
    "        # comparison.to_csv(output_file, index=False)\n",
    "        # print(f\"\\n对比结果已保存到: {output_file}\")\n",
    "        \n",
    "        # 显示统计摘要\n",
    "        print(\"\\n=== 统计摘要 ===\")\n",
    "        print(f\"总站点数: {len(comparison)-1}\")  # 减去汇总行\n",
    "        print(f\"完全被过滤的站点数: {(comparison['count_after'] == 0).sum() - 1}\")  # 减去汇总行\n",
    "        print(f\"平均保留率: {comparison[comparison['stationid'] != 'TOTAL']['retention_rate(%)'].mean():.2f}%\")\n",
    "        print(f\"\\n保留数据最多的前10个站点:\")\n",
    "        print(comparison.head(11)[['stationid', 'count_before', 'count_after', 'retention_rate(%)']])\n",
    "        \n",
    "        # 保存过滤后的数据\n",
    "        df.to_csv(f'./2.swot_node_qa{model}_{data_v}.csv', index=False)\n",
    "        print(f\"\\n过滤后的数据已保存到: ./2.swot_node_qa{model}_{data_v}.csv\")\n",
    "        \n",
    "        result = df.groupby(['date', 'stationid','node_id','reach_id','COMID'])[['wse', 'wse_u', 'width','width_u']].mean().reset_index()\n",
    "        result.to_csv(f'2.swot_datemean_qa{model}_{data_v}.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
