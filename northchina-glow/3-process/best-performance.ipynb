{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "28f70f5a-8c58-4023-b1bc-d4b7fa7009bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 64 workers for parallel processing\n",
      "Loading common data...\n",
      "\n",
      "Total configurations to run: 4\n",
      "Configurations:\n",
      "  ('node', '1.0', 'noqa', 'VersionD')\n",
      "  ('node', '1.0', 'qaloose', 'VersionD')\n",
      "  ('node', '1.5', 'noqa', 'VersionD')\n",
      "  ('node', '1.5', 'qaloose', 'VersionD')\n",
      "\n",
      "============================================================\n",
      "Running configuration: A=node, B=1.0, C=noqa, D=VersionD\n",
      "============================================================\n",
      "Step 1: Calculating width statistics using IQR method...\n",
      "Step 2: Selecting best nodes...\n",
      "Original nodes: 61, Selected nodes: 21\n",
      "Step 3: Applying quality control...\n",
      "\n",
      "[DIAGNOSTIC] Stations after QC (Step 2): 17\n",
      "Step 4: Fitting hydraulic curves...\n",
      "\n",
      "============================================================\n",
      "DIAGNOSTIC: Fitting 17 stations from QC data\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "DIAGNOSTIC REPORT: Station Drop Analysis (QC -> Fit)\n",
      "============================================================\n",
      "Total stations in QC data: 17\n",
      "Successfully fitted: 14\n",
      "Dropped stations: 3\n",
      "\n",
      "Drop reasons breakdown:\n",
      "----------------------------------------\n",
      "  all_fits_failed: 2 stations\n",
      "  too_few_points_after_width_filter: 1 stations\n",
      "\n",
      "\n",
      "TOO_FEW_POINTS_AFTER_WIDTH_FILTER (showing first 5 examples):\n",
      "----------------------------------------\n",
      "  Station: dalai\n",
      "    COMID: 42047856\n",
      "    Details: Original points: 7, After width filter: 1 (< 3 required), w_low=215.51, w_high=390.90, width range in data: [387.84, 701.78]\n",
      "\n",
      "ALL_FITS_FAILED (showing first 5 examples):\n",
      "----------------------------------------\n",
      "  Station: jiamusi\n",
      "    COMID: 42037498\n",
      "    Details: All 75 parameter combinations failed. Failures: ab_negative=0, ls_failed=75, exception=0\n",
      "  Station: kuerbin\n",
      "    COMID: 42026066\n",
      "    Details: All 75 parameter combinations failed. Failures: ab_negative=0, ls_failed=75, exception=0\n",
      "\n",
      "[DIAGNOSTIC] Stations after Fitting (Step 3): 14\n",
      "[DIAGNOSTIC] Station loss from QC to Fit: 3\n",
      "Step 5: Generating hypsometric curves...\n",
      "Step 6: Validating model...\n",
      "['changjiangtun', 'fuyu', 'heishiguan', 'huaibin', 'humaqiao', 'lianhua', 'liujiatun', 'luanxian', 'shihuiyao', 'shuanghetun', 'tangmazhai', 'tonghe', 'tonghua', 'xingjiawopeng']\n",
      "Configuration node_1.0_noqa_VersionD completed!\n",
      "Time for (node, 1.0, noqa, VersionD): 8.45s\n",
      "\n",
      "============================================================\n",
      "Running configuration: A=node, B=1.0, C=qaloose, D=VersionD\n",
      "============================================================\n",
      "Step 1: Calculating width statistics using IQR method...\n",
      "Step 2: Selecting best nodes...\n",
      "Original nodes: 36, Selected nodes: 15\n",
      "Step 3: Applying quality control...\n",
      "\n",
      "[DIAGNOSTIC] Stations after QC (Step 2): 13\n",
      "Step 4: Fitting hydraulic curves...\n",
      "\n",
      "============================================================\n",
      "DIAGNOSTIC: Fitting 13 stations from QC data\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "DIAGNOSTIC REPORT: Station Drop Analysis (QC -> Fit)\n",
      "============================================================\n",
      "Total stations in QC data: 13\n",
      "Successfully fitted: 9\n",
      "Dropped stations: 4\n",
      "\n",
      "Drop reasons breakdown:\n",
      "----------------------------------------\n",
      "  all_fits_failed: 3 stations\n",
      "  too_few_points_after_width_filter: 1 stations\n",
      "\n",
      "\n",
      "TOO_FEW_POINTS_AFTER_WIDTH_FILTER (showing first 5 examples):\n",
      "----------------------------------------\n",
      "  Station: changjiangtun\n",
      "    COMID: 42043227\n",
      "    Details: Original points: 8, After width filter: 0 (< 3 required), w_low=157.66, w_high=222.13, width range in data: [246.35, 291.48]\n",
      "\n",
      "ALL_FITS_FAILED (showing first 5 examples):\n",
      "----------------------------------------\n",
      "  Station: longmenzhen\n",
      "    COMID: 43027970\n",
      "    Details: All 75 parameter combinations failed. Failures: ab_negative=3, ls_failed=72, exception=0\n",
      "  Station: jiamusi\n",
      "    COMID: 42037498\n",
      "    Details: All 75 parameter combinations failed. Failures: ab_negative=0, ls_failed=75, exception=0\n",
      "  Station: kuerbin\n",
      "    COMID: 42026066\n",
      "    Details: All 75 parameter combinations failed. Failures: ab_negative=0, ls_failed=75, exception=0\n",
      "\n",
      "[DIAGNOSTIC] Stations after Fitting (Step 3): 9\n",
      "[DIAGNOSTIC] Station loss from QC to Fit: 4\n",
      "Step 5: Generating hypsometric curves...\n",
      "Step 6: Validating model...\n",
      "['fuyu', 'heishiguan', 'humaqiao', 'lianhua', 'liujiatun', 'shihuiyao', 'shuanghetun', 'tonghe', 'tonghua']\n",
      "Configuration node_1.0_qaloose_VersionD completed!\n",
      "Time for (node, 1.0, qaloose, VersionD): 5.22s\n",
      "\n",
      "============================================================\n",
      "Running configuration: A=node, B=1.5, C=noqa, D=VersionD\n",
      "============================================================\n",
      "Step 1: Calculating width statistics using IQR method...\n",
      "Step 2: Selecting best nodes...\n",
      "Original nodes: 61, Selected nodes: 21\n",
      "Step 3: Applying quality control...\n",
      "\n",
      "[DIAGNOSTIC] Stations after QC (Step 2): 17\n",
      "Step 4: Fitting hydraulic curves...\n",
      "\n",
      "============================================================\n",
      "DIAGNOSTIC: Fitting 17 stations from QC data\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "DIAGNOSTIC REPORT: Station Drop Analysis (QC -> Fit)\n",
      "============================================================\n",
      "Total stations in QC data: 17\n",
      "Successfully fitted: 15\n",
      "Dropped stations: 2\n",
      "\n",
      "Drop reasons breakdown:\n",
      "----------------------------------------\n",
      "  all_fits_failed: 2 stations\n",
      "\n",
      "\n",
      "ALL_FITS_FAILED (showing first 5 examples):\n",
      "----------------------------------------\n",
      "  Station: jiamusi\n",
      "    COMID: 42037498\n",
      "    Details: All 75 parameter combinations failed. Failures: ab_negative=0, ls_failed=75, exception=0\n",
      "  Station: kuerbin\n",
      "    COMID: 42026066\n",
      "    Details: All 75 parameter combinations failed. Failures: ab_negative=0, ls_failed=75, exception=0\n",
      "\n",
      "[DIAGNOSTIC] Stations after Fitting (Step 3): 15\n",
      "[DIAGNOSTIC] Station loss from QC to Fit: 2\n",
      "Step 5: Generating hypsometric curves...\n",
      "Step 6: Validating model...\n",
      "['changjiangtun', 'dalai', 'fuyu', 'heishiguan', 'huaibin', 'humaqiao', 'lianhua', 'liujiatun', 'luanxian', 'shihuiyao', 'shuanghetun', 'tangmazhai', 'tonghe', 'tonghua', 'xingjiawopeng']\n",
      "Configuration node_1.5_noqa_VersionD completed!\n",
      "Time for (node, 1.5, noqa, VersionD): 5.41s\n",
      "\n",
      "============================================================\n",
      "Running configuration: A=node, B=1.5, C=qaloose, D=VersionD\n",
      "============================================================\n",
      "Step 1: Calculating width statistics using IQR method...\n",
      "Step 2: Selecting best nodes...\n",
      "Original nodes: 36, Selected nodes: 15\n",
      "Step 3: Applying quality control...\n",
      "\n",
      "[DIAGNOSTIC] Stations after QC (Step 2): 13\n",
      "Step 4: Fitting hydraulic curves...\n",
      "\n",
      "============================================================\n",
      "DIAGNOSTIC: Fitting 13 stations from QC data\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "DIAGNOSTIC REPORT: Station Drop Analysis (QC -> Fit)\n",
      "============================================================\n",
      "Total stations in QC data: 13\n",
      "Successfully fitted: 9\n",
      "Dropped stations: 4\n",
      "\n",
      "Drop reasons breakdown:\n",
      "----------------------------------------\n",
      "  all_fits_failed: 3 stations\n",
      "  too_few_points_after_width_filter: 1 stations\n",
      "\n",
      "\n",
      "TOO_FEW_POINTS_AFTER_WIDTH_FILTER (showing first 5 examples):\n",
      "----------------------------------------\n",
      "  Station: changjiangtun\n",
      "    COMID: 42043227\n",
      "    Details: Original points: 8, After width filter: 0 (< 3 required), w_low=146.91, w_high=232.87, width range in data: [246.35, 291.48]\n",
      "\n",
      "ALL_FITS_FAILED (showing first 5 examples):\n",
      "----------------------------------------\n",
      "  Station: longmenzhen\n",
      "    COMID: 43027970\n",
      "    Details: All 75 parameter combinations failed. Failures: ab_negative=0, ls_failed=75, exception=0\n",
      "  Station: jiamusi\n",
      "    COMID: 42037498\n",
      "    Details: All 75 parameter combinations failed. Failures: ab_negative=0, ls_failed=75, exception=0\n",
      "  Station: kuerbin\n",
      "    COMID: 42026066\n",
      "    Details: All 75 parameter combinations failed. Failures: ab_negative=0, ls_failed=75, exception=0\n",
      "\n",
      "[DIAGNOSTIC] Stations after Fitting (Step 3): 9\n",
      "[DIAGNOSTIC] Station loss from QC to Fit: 4\n",
      "Step 5: Generating hypsometric curves...\n",
      "Step 6: Validating model...\n",
      "['fuyu', 'heishiguan', 'humaqiao', 'lianhua', 'liujiatun', 'shihuiyao', 'shuanghetun', 'tonghe', 'tonghua']\n",
      "Configuration node_1.5_qaloose_VersionD completed!\n",
      "Time for (node, 1.5, qaloose, VersionD): 5.19s\n",
      "\n",
      "Generating boxplot comparisons...\n",
      "\n",
      "Total time: 24.98s\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import linregress, spearmanr\n",
    "from scipy.optimize import least_squares\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import os\n",
    "import warnings\n",
    "import multiprocessing as mp\n",
    "from functools import partial\n",
    "import itertools\n",
    "from numba import jit, prange\n",
    "import gc\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ============================================================================\n",
    "# 全局配置\n",
    "# ============================================================================\n",
    "N_WORKERS = min(mp.cpu_count() - 1, 64)  # 80\n",
    "print(f\"Using {N_WORKERS} workers for parallel processing\")\n",
    "\n",
    "# ============================================================================\n",
    "# 全局变量（用于多进程共享数据）\n",
    "# ============================================================================\n",
    "_GLOBAL_NODE_DATA = {}\n",
    "_GLOBAL_WIDTH_STATS = None\n",
    "_GLOBAL_SWOT_DATA = None\n",
    "_GLOBAL_FITTER = None\n",
    "_GLOBAL_QC_DATA = None\n",
    "\n",
    "# 新增：用于收集诊断信息的全局变量\n",
    "_GLOBAL_DROP_REASONS = None\n",
    "\n",
    "# ============================================================================\n",
    "# Numba加速的核心计算函数\n",
    "# ============================================================================\n",
    "@jit(nopython=True, parallel=True, cache=True)\n",
    "def compute_inconsistency_matrix(w, h):\n",
    "    \"\"\"使用Numba加速计算不一致性矩阵\"\"\"\n",
    "    n = len(w)\n",
    "    inverse = np.zeros(n, dtype=np.int64)\n",
    "    for i in prange(n):\n",
    "        count = 0\n",
    "        for j in range(n):\n",
    "            w_diff = w[i] - w[j]\n",
    "            h_diff = h[i] - h[j]\n",
    "            if w_diff * h_diff < 0:\n",
    "                count += 1\n",
    "        inverse[i] = count\n",
    "    return inverse\n",
    "\n",
    "@jit(nopython=True, cache=True)\n",
    "def calculate_areas_numba(w_list, h_list, w50, a50):\n",
    "    \"\"\"使用Numba加速面积计算\"\"\"\n",
    "    n = len(w_list)\n",
    "    areas = np.full(n, np.nan)\n",
    "    \n",
    "    # 边界检查：如果数据点太少，直接返回\n",
    "    if n < 2:\n",
    "        return areas\n",
    "    \n",
    "    idx50 = np.searchsorted(w_list, w50)\n",
    "    if idx50 >= n:\n",
    "        idx50 = n - 1\n",
    "    if idx50 < 1:\n",
    "        idx50 = 1\n",
    "    \n",
    "    # 防止除零错误\n",
    "    denom = w_list[idx50] - w_list[idx50-1]\n",
    "    if abs(denom) < 1e-10:\n",
    "        # 如果宽度差太小，使用平均值\n",
    "        h50 = (h_list[idx50-1] + h_list[idx50]) / 2.0\n",
    "    else:\n",
    "        h50 = (h_list[idx50-1] * (w_list[idx50] - w50) +\n",
    "               h_list[idx50] * (w50 - w_list[idx50-1])) / denom\n",
    "    \n",
    "    areas[idx50] = a50 + 0.5 * (w50 + w_list[idx50]) * (h_list[idx50] - h50)\n",
    "    \n",
    "    for i in range(idx50 + 1, n):\n",
    "        areas[i] = areas[i-1] + 0.5 * (w_list[i-1] + w_list[i]) * \\\n",
    "                  (h_list[i] - h_list[i-1])\n",
    "    \n",
    "    for i in range(idx50 - 1, -1, -1):\n",
    "        areas[i] = areas[i+1] - 0.5 * (w_list[i+1] + w_list[i]) * \\\n",
    "                  (h_list[i+1] - h_list[i])\n",
    "    \n",
    "    return areas\n",
    "\n",
    "@jit(nopython=True, cache=True)\n",
    "def nse_numba(observed, simulated):\n",
    "    \"\"\"Numba加速的NSE计算\"\"\"\n",
    "    obs_mean = np.mean(observed)\n",
    "    numerator = np.sum((observed - simulated)**2)\n",
    "    denominator = np.sum((observed - obs_mean)**2)\n",
    "    if denominator == 0:\n",
    "        return np.nan\n",
    "    return 1 - numerator / denominator\n",
    "\n",
    "@jit(nopython=True, cache=True)\n",
    "def kge_numba(observed, simulated):\n",
    "    \"\"\"Numba加速的KGE计算\"\"\"\n",
    "    obs_mean = np.mean(observed)\n",
    "    sim_mean = np.mean(simulated)\n",
    "    obs_std = np.std(observed)\n",
    "    sim_std = np.std(simulated)\n",
    "    \n",
    "    if obs_std == 0 or sim_std == 0 or obs_mean == 0 or sim_mean == 0:\n",
    "        return np.nan\n",
    "    \n",
    "    n = len(observed)\n",
    "    cov = np.sum((observed - obs_mean) * (simulated - sim_mean)) / n\n",
    "    r = cov / (obs_std * sim_std)\n",
    "    \n",
    "    alpha = sim_mean / obs_mean\n",
    "    beta = (sim_std / sim_mean) / (obs_std / obs_mean)  # 变异系数比\n",
    "    \n",
    "    return 1 - np.sqrt((r - 1)**2 + (alpha - 1)**2 + (beta - 1)**2)\n",
    "\n",
    "# ============================================================================\n",
    "# 全局并行处理函数（必须在模块级别定义才能被pickle）\n",
    "# ============================================================================\n",
    "def _compute_node_corr(node_id):\n",
    "    \"\"\"计算单个节点的秩相关系数\"\"\"\n",
    "    global _GLOBAL_NODE_DATA\n",
    "    data = _GLOBAL_NODE_DATA.get(node_id)\n",
    "    if data is None or len(data['width']) < 5:\n",
    "        return (node_id, data['stationid'] if data else None, 0.0)\n",
    "    \n",
    "    try:\n",
    "        corr, _ = spearmanr(data['width'], data['wse'])\n",
    "        if np.isnan(corr):\n",
    "            corr = 0.0\n",
    "    except:\n",
    "        corr = 0.0\n",
    "    \n",
    "    return (node_id, data['stationid'], corr)\n",
    "\n",
    "def _process_qc_station(stationid):\n",
    "    \"\"\"处理单个站点的质控（全局函数版本）\"\"\"\n",
    "    global _GLOBAL_WIDTH_STATS, _GLOBAL_SWOT_DATA\n",
    "    \n",
    "    if _GLOBAL_WIDTH_STATS is None or stationid not in _GLOBAL_WIDTH_STATS.index:\n",
    "        return None\n",
    "    \n",
    "    df = _GLOBAL_SWOT_DATA[_GLOBAL_SWOT_DATA['stationid'] == stationid].copy()\n",
    "    if len(df) < 5:\n",
    "        return None\n",
    "    \n",
    "    # 步骤1: 不确定度筛选\n",
    "    df['width_u_r'] = df['width_u'] / df['width']\n",
    "    df1 = df[(df['wse_u'] <= 0.4) & (df['width_u_r'] <= 0.1)]\n",
    "    if len(df1) < 5:\n",
    "        return None\n",
    "    \n",
    "    # 步骤2: 顺序一致性剔除\n",
    "    df2 = _remove_inconsistent_points(df1)\n",
    "    if len(df2) < 5:\n",
    "        return None\n",
    "    \n",
    "    # 步骤3: 离群值剔除\n",
    "    w_low = _GLOBAL_WIDTH_STATS.loc[stationid, 'w_low']\n",
    "    w_high = _GLOBAL_WIDTH_STATS.loc[stationid, 'w_high']\n",
    "    d_bankfull = 0.27 * (w_high / 7.2) ** 0.6\n",
    "    h50 = df2['wse'].median()\n",
    "    \n",
    "    df3 = df2[(df2['wse'] <= h50 + d_bankfull) & (df2['wse'] >= h50 - d_bankfull)]\n",
    "    \n",
    "    return df3 if len(df3) >= 5 else None\n",
    "\n",
    "def _remove_inconsistent_points(df, inverse_ratio_thresh=0.5):\n",
    "    \"\"\"顺序一致性剔除\"\"\"\n",
    "    indices_to_keep = list(df.index)\n",
    "    \n",
    "    while True:\n",
    "        n = len(indices_to_keep)\n",
    "        if n < 5:\n",
    "            break\n",
    "        \n",
    "        df_current = df.loc[indices_to_keep]\n",
    "        w = df_current['width'].values.astype(np.float64)\n",
    "        h = df_current['wse'].values.astype(np.float64)\n",
    "        \n",
    "        inverse = compute_inconsistency_matrix(w, h)\n",
    "        \n",
    "        idx_max = np.argmax(inverse)\n",
    "        if inverse[idx_max] / n < inverse_ratio_thresh:\n",
    "            break\n",
    "        \n",
    "        indices_to_keep.pop(idx_max)\n",
    "    \n",
    "    return df.loc[indices_to_keep]\n",
    "\n",
    "def _fit_station_wrapper(stationid):\n",
    "    \"\"\"拟合单个站点（全局函数版本）- 带诊断信息\"\"\"\n",
    "    global _GLOBAL_FITTER, _GLOBAL_QC_DATA\n",
    "    \n",
    "    df_station = _GLOBAL_QC_DATA[_GLOBAL_QC_DATA['stationid'] == stationid]\n",
    "    if len(df_station) == 0:\n",
    "        return (None, {'stationid': stationid, 'reason': 'no_data_in_qc', 'details': 'Station not found in QC data'})\n",
    "    \n",
    "    comid = df_station.iloc[0]['COMID']\n",
    "    result, drop_info = _GLOBAL_FITTER.fit_station_with_diagnostics(df_station, stationid, comid)\n",
    "    return (result, drop_info)\n",
    "\n",
    "import glob  # 在文件顶部添加\n",
    "\n",
    "def _validate_station_wrapper(args):\n",
    "    \"\"\"验证单个站点（全局函数版本）\"\"\"\n",
    "    s, df_hypso, df_width, df_val_folder, df_fit, skip_width_filter = args\n",
    "    \n",
    "    # 【修复1】使用 glob 匹配文件\n",
    "    file_pattern = os.path.join(df_val_folder, f'{s}*.csv')\n",
    "    matching_files = glob.glob(file_pattern)\n",
    "    \n",
    "    if not matching_files:\n",
    "        return None\n",
    "    \n",
    "    file_path = matching_files[0]\n",
    "    \n",
    "    try:\n",
    "        df_val = pd.read_csv(file_path)\n",
    "        df_val = df_val.rename(columns={'Q': 'qobs'})\n",
    "        df_val['stationid'] = s\n",
    "        df_val = df_val.dropna(subset=['qobs'])\n",
    "        \n",
    "        # 【修复2】统一 date 列类型为 datetime\n",
    "        df_val['date'] = pd.to_datetime(df_val['date'])\n",
    "        \n",
    "        df_width_s = df_width[df_width['stationid'] == s].copy()\n",
    "        df_width_s['date'] = pd.to_datetime(df_width_s['date'])\n",
    "        \n",
    "        df_val = df_val.merge(df_width_s, on=['stationid', 'date'], how='inner')\n",
    "        \n",
    "        df_curve = df_hypso[df_hypso['stationid'] == s].reset_index(drop=True)\n",
    "        station_fit = df_fit[df_fit['stationid'] == s]\n",
    "        if station_fit.empty or df_curve.empty:\n",
    "            return None\n",
    "        \n",
    "        row = station_fit.iloc[0]\n",
    "        w_low, w_high, slp = row['w_low'], row['w_high'], row['slp']\n",
    "        \n",
    "        # 【修复3】处理合并后可能产生的 width_x/width_y 列名冲突\n",
    "        width_col = 'width_x' if 'width_x' in df_val.columns else 'width'\n",
    "        \n",
    "        if skip_width_filter:\n",
    "            df_val = df_val.drop_duplicates('date')\n",
    "        else:\n",
    "            df_val = df_val[\n",
    "                (df_val[width_col] >= w_low) &\n",
    "                (df_val[width_col] <= w_high)\n",
    "            ].drop_duplicates('date')\n",
    "        \n",
    "        if len(df_val) < 10:\n",
    "            return None\n",
    "        \n",
    "        curve_width = df_curve['width'].values\n",
    "        curve_area = df_curve['area'].values\n",
    "        val_width = df_val[width_col].values\n",
    "        \n",
    "        area_hypso = np.interp(val_width, curve_width, curve_area)\n",
    "        \n",
    "        df_val['area_hypso'] = area_hypso\n",
    "        df_val['Q_est'] = (area_hypso**(5/3) * val_width**(-2/3) * slp**0.5 / 0.035)\n",
    "        \n",
    "        df_val = df_val.dropna()\n",
    "        if len(df_val) < 10:\n",
    "            return None\n",
    "        \n",
    "        obs = df_val['qobs'].values.astype(np.float64)\n",
    "        sim = df_val['Q_est'].values.astype(np.float64)\n",
    "        \n",
    "        kge_val = kge_numba(obs, sim)\n",
    "        nse_val = nse_numba(obs, sim)\n",
    "        rmse = np.sqrt(np.mean((obs - sim)**2))\n",
    "        nrmse_val = rmse / np.mean(obs)\n",
    "        \n",
    "        df_val['kge'] = kge_val\n",
    "        df_val['nse'] = nse_val\n",
    "        df_val['nrmse'] = nrmse_val\n",
    "        \n",
    "        # 【修复4】返回时重命名 width 列\n",
    "        return df_val[['stationid', 'date', width_col, 'area_hypso',\n",
    "                       'qobs', 'Q_est', 'kge', 'nse', 'nrmse']].rename(\n",
    "                           columns={width_col: 'width'})\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing station {s}: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def _rolling_median_group(group):\n",
    "    \"\"\"滑动中值处理\"\"\"\n",
    "    group = group.sort_values('date')\n",
    "    group['width'] = group['width'].rolling(window=5, center=True, min_periods=1).median()\n",
    "    group['wse'] = group['wse'].rolling(window=5, center=True, min_periods=1).median()\n",
    "    return group\n",
    "\n",
    "# ============================================================================\n",
    "# 模块1: 数据统计工具 (修改为IQR方法)\n",
    "# ============================================================================\n",
    "class WidthStatistics:\n",
    "    \"\"\"计算河流宽度的统计特征 - 使用IQR方法\"\"\"\n",
    "    \n",
    "    # 定义IQR配置：{选项: IQR倍数}\n",
    "    # w_low = Q1 - k * IQR\n",
    "    # w_high = Q3 + k * IQR\n",
    "    IQR_CONFIG = {\n",
    "        '1.0': 1.0,\n",
    "        '1.5': 1.5,\n",
    "        '2.0': 2.0,\n",
    "        '2.5': 2.5,\n",
    "        '3.0': 3.0,\n",
    "        '4.0': 4.0\n",
    "    }\n",
    "    \n",
    "    @staticmethod\n",
    "    def calculate_width_iqr(df, min_width=30, valid_ratio=0.95, min_iqr=5):\n",
    "        \"\"\"\n",
    "        计算每个站点的宽度IQR范围\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        df : DataFrame\n",
    "            输入数据\n",
    "        min_width : float\n",
    "            最小有效宽度\n",
    "        valid_ratio : float\n",
    "            有效数据比例阈值\n",
    "        min_iqr : float\n",
    "            最小IQR阈值，当IQR小于此值时跳过该站点\n",
    "        \"\"\"\n",
    "        stationids = df['stationid'].unique()\n",
    "        result_data = []\n",
    "        skipped_stations = []\n",
    "        \n",
    "        for stationid in stationids:\n",
    "            station_data_all = df[df['stationid'] == stationid]['width'].dropna()\n",
    "            station_data = station_data_all[station_data_all >= min_width]\n",
    "            \n",
    "            if len(station_data_all) == 0:\n",
    "                continue\n",
    "            if len(station_data) / len(station_data_all) < valid_ratio:\n",
    "                continue\n",
    "            \n",
    "            if len(station_data) > 10:\n",
    "                w50 = station_data.median()\n",
    "                \n",
    "                # 计算Q1, Q3和IQR\n",
    "                q1 = station_data.quantile(0.25)\n",
    "                q3 = station_data.quantile(0.75)\n",
    "                iqr = q3 - q1\n",
    "                \n",
    "                # 检查IQR是否足够大，如果Q1和Q3太接近则跳过该站点\n",
    "                if iqr < min_iqr:\n",
    "                    skipped_stations.append((stationid, q1, q3, iqr))\n",
    "                    continue\n",
    "                \n",
    "                row_data = {\n",
    "                    'stationid': stationid,\n",
    "                    'w50': w50,\n",
    "                    'Q1': q1,\n",
    "                    'Q3': q3,\n",
    "                    'IQR': iqr\n",
    "                }\n",
    "                \n",
    "                # 动态计算所有IQR倍数配置的范围\n",
    "                for key, k in WidthStatistics.IQR_CONFIG.items():\n",
    "                    w_low = max(q1 - k * iqr, min_width)  # 确保不低于最小宽度\n",
    "                    w_high = q3 + k * iqr\n",
    "                    row_data[f'w_low_iqr{key}'] = w_low\n",
    "                    row_data[f'w_high_iqr{key}'] = w_high\n",
    "                \n",
    "                result_data.append(row_data)\n",
    "        \n",
    "        # 打印跳过的站点信息\n",
    "        if skipped_stations:\n",
    "            print(f\"Skipped {len(skipped_stations)} stations due to small IQR (< {min_iqr}):\")\n",
    "            for sid, q1, q3, iqr in skipped_stations[:10]:  # 只打印前10个\n",
    "                print(f\"  Station {sid}: Q1={q1:.2f}, Q3={q3:.2f}, IQR={iqr:.2f}\")\n",
    "            if len(skipped_stations) > 10:\n",
    "                print(f\"  ... and {len(skipped_stations) - 10} more stations\")\n",
    "        \n",
    "        return pd.DataFrame(result_data)\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_iqr_columns(b_option):\n",
    "        \"\"\"根据B选项获取对应的IQR列名\"\"\"\n",
    "        if b_option not in WidthStatistics.IQR_CONFIG:\n",
    "            raise ValueError(f\"Invalid B option: {b_option}. Valid options: {list(WidthStatistics.IQR_CONFIG.keys())}\")\n",
    "        \n",
    "        return f'w_low_iqr{b_option}', f'w_high_iqr{b_option}'\n",
    "    \n",
    "    # 保留旧方法以兼容（如果需要）\n",
    "    @staticmethod\n",
    "    def calculate_width_percentiles(df, min_width=30, valid_ratio=0.95, min_iqr=5):\n",
    "        \"\"\"计算每个站点的宽度分位数（保留用于兼容）\"\"\"\n",
    "        return WidthStatistics.calculate_width_iqr(df, min_width, valid_ratio, min_iqr)\n",
    "\n",
    "# ============================================================================\n",
    "# 模块2: 节点选择\n",
    "# ============================================================================\n",
    "class NodeSelector:\n",
    "    \"\"\"为每个站点选择最优节点\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def select_best_nodes(df_swot, min_data_points=10):\n",
    "        \"\"\"为每个站点选择秩相关系数最大的节点\"\"\"\n",
    "        global _GLOBAL_NODE_DATA\n",
    "        \n",
    "        # 向量化计算节点数据量\n",
    "        node_counts = df_swot.groupby('node_id').size()\n",
    "        valid_nodes = node_counts[node_counts >= min_data_points].index\n",
    "        df_swot = df_swot[df_swot['node_id'].isin(valid_nodes)].copy()\n",
    "        \n",
    "        # 预计算每个节点的数据\n",
    "        _GLOBAL_NODE_DATA = {}\n",
    "        for node_id, group in df_swot.groupby('node_id'):\n",
    "            _GLOBAL_NODE_DATA[node_id] = {\n",
    "                'width': group['width'].values,\n",
    "                'wse': group['wse'].values,\n",
    "                'stationid': group['stationid'].iloc[0]\n",
    "            }\n",
    "        \n",
    "        node_ids = list(_GLOBAL_NODE_DATA.keys())\n",
    "        \n",
    "        # 使用进程池并行计算\n",
    "        with mp.Pool(processes=N_WORKERS) as pool:\n",
    "            results = pool.map(_compute_node_corr, node_ids)\n",
    "        \n",
    "        df_node = pd.DataFrame(results, columns=['node_id', 'stationid', 'rank_corr'])\n",
    "        df_node = df_node.dropna(subset=['stationid'])\n",
    "        \n",
    "        # 选择每个站点的最大秩相关系数节点\n",
    "        max_idx = df_node.groupby('stationid')['rank_corr'].idxmax()\n",
    "        df_node_rmax = df_node.loc[max_idx]\n",
    "        \n",
    "        # 筛选数据\n",
    "        df_filtered = df_swot[df_swot['node_id'].isin(df_node_rmax['node_id'])]\n",
    "        \n",
    "        print(f\"Original nodes: {len(df_node)}, Selected nodes: {len(df_node_rmax)}\")\n",
    "        \n",
    "        # 清理全局变量\n",
    "        _GLOBAL_NODE_DATA = {}\n",
    "        \n",
    "        return df_filtered, df_node_rmax\n",
    "\n",
    "# ============================================================================\n",
    "# 模块3: 数据质控\n",
    "# ============================================================================\n",
    "class DataQualityControl:\n",
    "    \"\"\"SWOT数据质量控制\"\"\"\n",
    "    \n",
    "    def __init__(self, width_stats):\n",
    "        self.width_stats = width_stats.set_index('stationid')\n",
    "    \n",
    "    def apply_qc(self, df_swot, draw_figure=False, output_folder=None):\n",
    "        \"\"\"应用完整的质量控制流程\"\"\"\n",
    "        global _GLOBAL_WIDTH_STATS, _GLOBAL_SWOT_DATA\n",
    "        \n",
    "        _GLOBAL_WIDTH_STATS = self.width_stats\n",
    "        _GLOBAL_SWOT_DATA = df_swot\n",
    "        \n",
    "        stationids = df_swot['stationid'].unique()\n",
    "        \n",
    "        # 使用进程池并行处理\n",
    "        with mp.Pool(processes=N_WORKERS) as pool:\n",
    "            results = pool.map(_process_qc_station, stationids)\n",
    "        \n",
    "        # 清理全局变量\n",
    "        _GLOBAL_WIDTH_STATS = None\n",
    "        _GLOBAL_SWOT_DATA = None\n",
    "        \n",
    "        # 合并结果\n",
    "        results = [df for df in results if df is not None]\n",
    "        \n",
    "        if not results:\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        result = pd.concat(results)\n",
    "        result = result.drop_duplicates(subset=['node_id', 'date', 'stationid'])\n",
    "        result.reset_index(drop=True, inplace=True)\n",
    "        \n",
    "        return result\n",
    "\n",
    "# ============================================================================\n",
    "# 模块4: 水位-面积曲线拟合 (添加诊断功能)\n",
    "# ============================================================================\n",
    "class HydraulicCurveFitter:\n",
    "    \"\"\"拟合水位-宽度关系曲线\"\"\"\n",
    "    \n",
    "    def __init__(self, width_stats, river_attrs, skip_width_filter=False):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "        -----------\n",
    "        width_stats : DataFrame\n",
    "            宽度统计数据\n",
    "        river_attrs : DataFrame\n",
    "            河流属性数据\n",
    "        skip_width_filter : bool\n",
    "            是否跳过宽度筛选（用于datemean模式）\n",
    "        \"\"\"\n",
    "        self.width_stats = width_stats.set_index('stationid')\n",
    "        self.river_attrs = river_attrs.set_index('COMID')\n",
    "        self.skip_width_filter = skip_width_filter\n",
    "        \n",
    "        self.R_list = np.array([0.5, 1, 2, 4, 8])\n",
    "        self.GAP_list = np.array([-0.3,-0.1, 0, 0.1,0.3])\n",
    "        self.W_list = np.array([0.3, 0.5, 0.7])\n",
    "        \n",
    "        # 诊断信息收集\n",
    "        self.drop_reasons = []\n",
    "    \n",
    "    @staticmethod\n",
    "    def power_function(params, X, y):\n",
    "        wse0, a, b = params\n",
    "        return y - (wse0 + a * X**b)\n",
    "    \n",
    "    def loss_function(self, z, weight, n_swot):\n",
    "        rho = np.zeros((3, len(z)))\n",
    "        rho[0] = 2 * ((1 + z)**0.5 - 1)\n",
    "        rho[1] = (1 + z)**(-0.5)\n",
    "        rho[2] = -0.5 * (1 + z)**(-1.5)\n",
    "        \n",
    "        factor = (n_swot - 2) / weight * (1 - weight) / 2\n",
    "        rho[:, 0] *= factor\n",
    "        rho[:, 1] *= factor\n",
    "        \n",
    "        return rho\n",
    "    \n",
    "    def calculate_h50(self, df, w50):\n",
    "        df = df.copy()\n",
    "        df['w50_diff'] = np.abs(df['width'] - w50)\n",
    "        df = df.sort_values('w50_diff')\n",
    "        \n",
    "        xdata = df.iloc[:5]['width'].values\n",
    "        ydata = df.iloc[:5]['wse'].values\n",
    "        xdata_uni = np.unique(xdata)\n",
    "        \n",
    "        if len(xdata_uni) < 2:\n",
    "            return df.iloc[:5]['wse'].mean()\n",
    "        \n",
    "        res = linregress(xdata, ydata)\n",
    "        if res[0] >= 0:\n",
    "            return res[0] * w50 + res[1]\n",
    "        else:\n",
    "            return df.iloc[:5]['wse'].mean()\n",
    "    \n",
    "    def fit_station_with_diagnostics(self, df_station, stationid, comid):\n",
    "        \"\"\"拟合单个站点的水位-宽度关系 - 带诊断信息\"\"\"\n",
    "        drop_info = {'stationid': stationid, 'COMID': comid, 'reason': None, 'details': None}\n",
    "        \n",
    "        # 检查1: stationid是否在width_stats中\n",
    "        if stationid not in self.width_stats.index:\n",
    "            drop_info['reason'] = 'not_in_width_stats'\n",
    "            drop_info['details'] = f'Station {stationid} not found in width statistics'\n",
    "            return None, drop_info\n",
    "        \n",
    "        # 检查2: comid是否在river_attrs中\n",
    "        if comid not in self.river_attrs.index:\n",
    "            drop_info['reason'] = 'not_in_river_attrs'\n",
    "            drop_info['details'] = f'COMID {comid} not found in river attributes'\n",
    "            return None, drop_info\n",
    "        \n",
    "        try:\n",
    "            q50 = self.river_attrs.loc[comid, 'q50_weighted']\n",
    "            slp = self.river_attrs.loc[comid, 'slope']\n",
    "            w50, w_low, w_high = self.width_stats.loc[stationid, ['w50', 'w_low', 'w_high']]\n",
    "            d_bankfull = 0.27 * (w_high / 7.2)**0.6\n",
    "            \n",
    "            h50 = self.calculate_h50(df_station, w50)\n",
    "            a50 = (q50 * 0.035 / slp**0.5 * w50**(2/3))**(3/5)\n",
    "            \n",
    "            # 记录原始点数\n",
    "            original_count = len(df_station)\n",
    "            \n",
    "            # 根据skip_width_filter决定是否进行宽度筛选\n",
    "            if self.skip_width_filter:\n",
    "                # datemean模式：不对width做筛选\n",
    "                df_filtered = df_station.copy()\n",
    "            else:\n",
    "                # node模式：正常进行宽度筛选\n",
    "                df_filtered = df_station[\n",
    "                    (df_station['width'] >= w_low) &\n",
    "                    (df_station['width'] <= w_high)\n",
    "                ]\n",
    "            \n",
    "            filtered_count = len(df_filtered)\n",
    "            \n",
    "            # 检查3: 宽度筛选后数据点太少\n",
    "            if filtered_count < 3:\n",
    "                drop_info['reason'] = 'too_few_points_after_width_filter'\n",
    "                drop_info['details'] = (f'Original points: {original_count}, '\n",
    "                                        f'After width filter: {filtered_count} (< 3 required), '\n",
    "                                        f'w_low={w_low:.2f}, w_high={w_high:.2f}, '\n",
    "                                        f'width range in data: [{df_station[\"width\"].min():.2f}, {df_station[\"width\"].max():.2f}]')\n",
    "                return None, drop_info\n",
    "            \n",
    "            swot_wsemax = df_filtered.sort_values('wse', ascending=False).iloc[0]\n",
    "            d_wsemax = 0.27 * (swot_wsemax['width'] / 7.2)**0.6\n",
    "            \n",
    "            results = []\n",
    "            fit_failures = {'ab_negative': 0, 'ls_failed': 0, 'exception': 0}\n",
    "            \n",
    "            for r_low in self.R_list:\n",
    "                for gap in self.GAP_list:\n",
    "                    for weight in self.W_list:\n",
    "                        result, fail_reason = self._fit_single_config_with_diagnostics(\n",
    "                            df_filtered, r_low, gap, weight,\n",
    "                            w_low, w_high, w50, h50, a50,\n",
    "                            swot_wsemax, d_bankfull, d_wsemax, slp, q50\n",
    "                        )\n",
    "                        if result is not None:\n",
    "                            result.update({\n",
    "                                'stationid': stationid,\n",
    "                                'COMID': comid,\n",
    "                                'R': r_low,\n",
    "                                'GAP': gap,\n",
    "                                'W': weight\n",
    "                            })\n",
    "                            results.append(result)\n",
    "                        elif fail_reason:\n",
    "                            fit_failures[fail_reason] = fit_failures.get(fail_reason, 0) + 1\n",
    "            \n",
    "            # 检查4: 所有参数组合的拟合都失败\n",
    "            if not results:\n",
    "                total_configs = len(self.R_list) * len(self.GAP_list) * len(self.W_list)\n",
    "                drop_info['reason'] = 'all_fits_failed'\n",
    "                drop_info['details'] = (f'All {total_configs} parameter combinations failed. '\n",
    "                                        f'Failures: ab_negative={fit_failures[\"ab_negative\"]}, '\n",
    "                                        f'ls_failed={fit_failures[\"ls_failed\"]}, '\n",
    "                                        f'exception={fit_failures[\"exception\"]}')\n",
    "                return None, drop_info\n",
    "            \n",
    "            # 成功\n",
    "            drop_info['reason'] = 'success'\n",
    "            drop_info['details'] = f'Successfully fitted with {len(results)} parameter combinations'\n",
    "            return pd.DataFrame(results), drop_info\n",
    "            \n",
    "        except Exception as e:\n",
    "            drop_info['reason'] = 'exception'\n",
    "            drop_info['details'] = f'Exception during fitting: {str(e)}'\n",
    "            return None, drop_info\n",
    "    \n",
    "    def fit_station(self, df_station, stationid, comid):\n",
    "        \"\"\"拟合单个站点的水位-宽度关系（保持原有接口）\"\"\"\n",
    "        result, _ = self.fit_station_with_diagnostics(df_station, stationid, comid)\n",
    "        return result\n",
    "    \n",
    "    def _fit_single_config_with_diagnostics(self, df, r_low, gap, weight, w_low, w_high, w50,\n",
    "                          h50, a50, swot_wsemax, d_bankfull, d_wsemax, slp, q50):\n",
    "        \"\"\"拟合单个参数配置 - 带失败原因\"\"\"\n",
    "        try:\n",
    "            a_low = a50 * (r_low + 1) / r_low / w50**(r_low + 1)\n",
    "            h0 = h50 - a_low * w50**r_low\n",
    "            h_low = h0 + a_low * w_low**r_low\n",
    "            h_high = swot_wsemax['wse'] + (d_bankfull - d_wsemax) + gap * d_bankfull\n",
    "            \n",
    "            xdata = np.insert(df['width'].values, 0, [w_low, w_high])\n",
    "            ydata = np.insert(df['wse'].values, 0, [h_low, h_high])\n",
    "            a_default = (h_high - h0) / w_high**2\n",
    "            \n",
    "            n_swot = len(df)\n",
    "            \n",
    "            def loss_wrapper(z):\n",
    "                return self.loss_function(z, weight, n_swot)\n",
    "            \n",
    "            ls = least_squares(\n",
    "                self.power_function,\n",
    "                x0=[h0, a_default, 2],\n",
    "                loss=loss_wrapper,\n",
    "                args=(xdata, ydata),\n",
    "                max_nfev=100\n",
    "            )\n",
    "            \n",
    "            if ls.status > 0:\n",
    "                wse0, a, b = ls.x\n",
    "                if a * b < 0:\n",
    "                    return None, 'ab_negative'\n",
    "                \n",
    "                return {\n",
    "                    'wse0': wse0, 'a': a, 'b': b,\n",
    "                    'a50': a50, 'w50': w50, 'q50': q50,\n",
    "                    'w_low': w_low, 'w_high': w_high,\n",
    "                    'h_low': h_low, 'h_high': h_high,\n",
    "                    'slp': slp\n",
    "                }, None\n",
    "            else:\n",
    "                return None, 'ls_failed'\n",
    "        except Exception as e:\n",
    "            return None, 'exception'\n",
    "    \n",
    "    def _fit_single_config(self, df, r_low, gap, weight, w_low, w_high, w50,\n",
    "                          h50, a50, swot_wsemax, d_bankfull, d_wsemax, slp, q50):\n",
    "        \"\"\"拟合单个参数配置（保持原有接口）\"\"\"\n",
    "        result, _ = self._fit_single_config_with_diagnostics(\n",
    "            df, r_low, gap, weight, w_low, w_high, w50,\n",
    "            h50, a50, swot_wsemax, d_bankfull, d_wsemax, slp, q50\n",
    "        )\n",
    "        return result\n",
    "    \n",
    "    def fit_all_stations(self, df_qc):\n",
    "        \"\"\"并行拟合所有站点 - 带诊断输出\"\"\"\n",
    "        global _GLOBAL_FITTER, _GLOBAL_QC_DATA\n",
    "        \n",
    "        _GLOBAL_FITTER = self\n",
    "        _GLOBAL_QC_DATA = df_qc\n",
    "        \n",
    "        unique_stations = df_qc['stationid'].unique()\n",
    "        stations_in_qc = set(unique_stations)\n",
    "        \n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"DIAGNOSTIC: Fitting {len(unique_stations)} stations from QC data\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        # 使用进程池并行处理\n",
    "        with mp.Pool(processes=N_WORKERS) as pool:\n",
    "            results_with_info = pool.map(_fit_station_wrapper, unique_stations)\n",
    "        \n",
    "        # 清理全局变量\n",
    "        _GLOBAL_FITTER = None\n",
    "        _GLOBAL_QC_DATA = None\n",
    "        \n",
    "        # 分离结果和诊断信息\n",
    "        results = []\n",
    "        drop_infos = []\n",
    "        \n",
    "        for result, drop_info in results_with_info:\n",
    "            if result is not None:\n",
    "                results.append(result)\n",
    "            drop_infos.append(drop_info)\n",
    "        \n",
    "        # 生成诊断报告\n",
    "        self._generate_diagnostic_report(drop_infos, stations_in_qc)\n",
    "        \n",
    "        if results:\n",
    "            return pd.concat(results, ignore_index=True)\n",
    "        return None\n",
    "    \n",
    "    def _generate_diagnostic_report(self, drop_infos, stations_in_qc):\n",
    "        \"\"\"生成诊断报告\"\"\"\n",
    "        # 统计各种丢弃原因\n",
    "        reason_counts = {}\n",
    "        reason_details = {}\n",
    "        \n",
    "        for info in drop_infos:\n",
    "            reason = info['reason']\n",
    "            if reason not in reason_counts:\n",
    "                reason_counts[reason] = 0\n",
    "                reason_details[reason] = []\n",
    "            reason_counts[reason] += 1\n",
    "            if reason != 'success':\n",
    "                reason_details[reason].append({\n",
    "                    'stationid': info['stationid'],\n",
    "                    'COMID': info.get('COMID'),\n",
    "                    'details': info['details']\n",
    "                })\n",
    "        \n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(\"DIAGNOSTIC REPORT: Station Drop Analysis (QC -> Fit)\")\n",
    "        print(f\"{'='*60}\")\n",
    "        print(f\"Total stations in QC data: {len(stations_in_qc)}\")\n",
    "        print(f\"Successfully fitted: {reason_counts.get('success', 0)}\")\n",
    "        print(f\"Dropped stations: {len(stations_in_qc) - reason_counts.get('success', 0)}\")\n",
    "        print()\n",
    "        \n",
    "        print(\"Drop reasons breakdown:\")\n",
    "        print(\"-\" * 40)\n",
    "        for reason, count in sorted(reason_counts.items(), key=lambda x: -x[1]):\n",
    "            if reason != 'success':\n",
    "                print(f\"  {reason}: {count} stations\")\n",
    "        print()\n",
    "        \n",
    "        # 详细列出每种原因的前几个例子\n",
    "        for reason in ['not_in_width_stats', 'not_in_river_attrs', \n",
    "                       'too_few_points_after_width_filter', 'all_fits_failed', \n",
    "                       'exception', 'no_data_in_qc']:\n",
    "            if reason in reason_details and reason_details[reason]:\n",
    "                print(f\"\\n{reason.upper()} (showing first 5 examples):\")\n",
    "                print(\"-\" * 40)\n",
    "                for item in reason_details[reason][:5]:\n",
    "                    print(f\"  Station: {item['stationid']}\")\n",
    "                    print(f\"    COMID: {item['COMID']}\")\n",
    "                    print(f\"    Details: {item['details']}\")\n",
    "                if len(reason_details[reason]) > 5:\n",
    "                    print(f\"  ... and {len(reason_details[reason]) - 5} more stations\")\n",
    "        \n",
    "        # 保存完整诊断报告到CSV\n",
    "        df_diagnostics = pd.DataFrame(drop_infos)\n",
    "        return df_diagnostics\n",
    "\n",
    "# ============================================================================\n",
    "# 模块5: 水位-面积曲线生成\n",
    "# ============================================================================\n",
    "class HypsometricCurveGenerator:\n",
    "    \"\"\"生成水位-面积关系曲线\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def generate_curves(df_fit, n_points=100):\n",
    "        \"\"\"为所有站点生成中值水位-面积曲线\"\"\"\n",
    "        stationids = sorted(df_fit['stationid'].unique())\n",
    "        df_res = []\n",
    "        \n",
    "        for s in stationids:\n",
    "            df_station = df_fit[df_fit['stationid'] == s]\n",
    "            w_low, w_high, w50, a50 = df_station.iloc[0][\n",
    "                ['w_low', 'w_high', 'w50', 'a50']\n",
    "            ]\n",
    "            \n",
    "            # 边界检查：跳过无效的宽度范围\n",
    "            if w_high <= w_low or abs(w_high - w_low) < 1e-6:\n",
    "                print(f\"Warning: Skipping station {s} due to invalid width range (w_low={w_low}, w_high={w_high})\")\n",
    "                continue\n",
    "            \n",
    "            wse0 = df_station['wse0'].values\n",
    "            a = df_station['a'].values\n",
    "            b = df_station['b'].values\n",
    "            \n",
    "            w_list = np.linspace(w_low, w_high, n_points)\n",
    "            \n",
    "            # 向量化计算\n",
    "            heights_all = wse0[:, np.newaxis] + a[:, np.newaxis] * w_list**b[:, np.newaxis]\n",
    "            h_list = np.median(heights_all, axis=0)\n",
    "            hmax = np.max(heights_all, axis=0)\n",
    "            hmin = np.min(heights_all, axis=0)\n",
    "            \n",
    "            # Numba加速的面积计算\n",
    "            areas = calculate_areas_numba(w_list, h_list, w50, a50)\n",
    "            \n",
    "            df_curve = pd.DataFrame({\n",
    "                'stationid': s,\n",
    "                'width': w_list,\n",
    "                'wse': h_list,\n",
    "                'wse_max': hmax,\n",
    "                'wse_min': hmin,\n",
    "                'area': areas\n",
    "            })\n",
    "            \n",
    "            df_res.append(df_curve)\n",
    "        \n",
    "        return pd.concat(df_res, ignore_index=True) if df_res else pd.DataFrame()\n",
    "\n",
    "# ============================================================================\n",
    "# 模块6: 验证与评估\n",
    "# ============================================================================\n",
    "class ModelValidator:\n",
    "    \"\"\"模型验证与性能评估\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def relative_rmse(observed, simulated):\n",
    "        rmse = np.sqrt(mean_squared_error(observed, simulated))\n",
    "        return rmse / np.mean(observed)\n",
    "    \n",
    "    def validate(self, df_hypso, df_width, df_val_folder, df_fit, skip_width_filter=False):\n",
    "        \"\"\"\n",
    "        验证模型性能\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        skip_width_filter : bool\n",
    "            是否跳过宽度筛选（用于datemean模式）\n",
    "        \"\"\"\n",
    "        stationids = sorted(df_hypso['stationid'].unique())\n",
    "        print(stationids)\n",
    "        # 【修改】在参数列表中添加 skip_width_filter\n",
    "        args_list = [\n",
    "            (s, df_hypso, df_width, df_val_folder, df_fit,  skip_width_filter)\n",
    "            for s in stationids\n",
    "        ]\n",
    "        \n",
    "        # 使用进程池并行处理\n",
    "        with mp.Pool(processes=N_WORKERS) as pool:\n",
    "            results = pool.map(_validate_station_wrapper, args_list)\n",
    "        \n",
    "        results = [df for df in results if df is not None]\n",
    "        \n",
    "        return pd.concat(results, ignore_index=True) if results else pd.DataFrame()\n",
    "\n",
    "# ============================================================================\n",
    "# 配置运行函数\n",
    "# ============================================================================\n",
    "def run_configuration(a, b, c, d, common_data):\n",
    "    \"\"\"\n",
    "    运行单个配置\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    a : str\n",
    "        处理方式: 'node' 或 'datemean'\n",
    "    b : str\n",
    "        IQR倍数选项: '1.0', '1.5', '2.0', '2.5', '3.0', '4.0'\n",
    "    c : str\n",
    "        QA选项: 'noqa', 'qaloose', 'qastrict'\n",
    "    d : str\n",
    "        版本选项: 'VersionD', 'VersionC'\n",
    "    common_data : dict\n",
    "        共享数据\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Running configuration: A={a}, B={b}, C={c}, D={d}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    gc.collect()\n",
    "    \n",
    "    # 步骤1: 计算宽度统计（使用IQR方法）\n",
    "    print(\"Step 1: Calculating width statistics using IQR method...\")\n",
    "    df_l8 = common_data['df_l8']\n",
    "    df_w_stats = WidthStatistics.calculate_width_iqr(df_l8)\n",
    "    \n",
    "    # 使用IQR方法获取列名\n",
    "    low_col, high_col = WidthStatistics.get_iqr_columns(b)\n",
    "    df_w_stats['w_low'] = df_w_stats[low_col]\n",
    "    df_w_stats['w_high'] = df_w_stats[high_col]\n",
    "    \n",
    "    df_w_stats.to_csv(f'1.width_statistic_iqr_{a}_{b}_{c}_{d}.csv', index=False)\n",
    "    \n",
    "    df_comid = common_data['df_comid']\n",
    "    df_attrs = common_data['df_attrs']\n",
    "\n",
    "    # 根据c选项加载SWOT数据\n",
    "    if c == 'noqa':\n",
    "        # noqa时文件名固定\n",
    "        df_swot = pd.read_csv(f'1.all_matched_points_{d}.csv')\n",
    "    else:\n",
    "        # qaloose或qastrict时，文件名根据a确定\n",
    "        df_swot = pd.read_csv(f'2.swot_{a}_{c}_{d}.csv')\n",
    "   \n",
    "    df_swot = df_swot.merge(df_comid, on='stationid', how='inner')\n",
    "\n",
    "    if a == 'node':\n",
    "        print(\"Step 2: Selecting best nodes...\")\n",
    "        df_swot_filtered, df_nodes = NodeSelector.select_best_nodes(df_swot, min_data_points=10)\n",
    "        \n",
    "        print(\"Step 3: Applying quality control...\")\n",
    "        qc = DataQualityControl(df_w_stats)\n",
    "        df_qc = qc.apply_qc(df_swot_filtered, draw_figure=False)\n",
    "        \n",
    "    elif a == 'datemean':\n",
    "        print(\"Using smoothed data (skipping width filter)...\")\n",
    "        # 使用进程池并行处理滑动中值\n",
    "        groups = [group for _, group in df_swot.groupby('stationid')]\n",
    "        \n",
    "        with mp.Pool(processes=N_WORKERS) as pool:\n",
    "            results = pool.map(_rolling_median_group, groups)\n",
    "        \n",
    "        df_qc = pd.concat(results)\n",
    "    \n",
    "    if 'COMID' not in df_qc.columns:\n",
    "        df_qc = df_qc.merge(df_comid, on='stationid', how='left')\n",
    "    \n",
    "    cols = ['COMID'] + [col for col in df_qc.columns if col != 'COMID']\n",
    "    df_qc = df_qc[cols]\n",
    "    df_qc.to_csv(f'2.swot-points-selection_iqr_{a}_{b}_{c}_{d}.csv', index=False)\n",
    "    \n",
    "    # ========== 新增：输出QC阶段站点统计 ==========\n",
    "    stations_in_qc = df_qc['stationid'].unique()\n",
    "    print(f\"\\n[DIAGNOSTIC] Stations after QC (Step 2): {len(stations_in_qc)}\")\n",
    "    \n",
    "    # 步骤4: 拟合\n",
    "    print(\"Step 4: Fitting hydraulic curves...\")\n",
    "    # 根据a选项决定是否跳过宽度筛选\n",
    "    skip_width_filter = (a == 'datemean')\n",
    "    fitter = HydraulicCurveFitter(df_w_stats, df_attrs, skip_width_filter=skip_width_filter)\n",
    "    df_fit_all = fitter.fit_all_stations(df_qc)\n",
    "    \n",
    "    if df_fit_all is None or len(df_fit_all) == 0:\n",
    "        print(f\"No fit data for {a}_{b}_{c}_{d}\")\n",
    "        return\n",
    "    \n",
    "    # ========== 新增：输出拟合阶段站点统计 ==========\n",
    "    stations_in_fit = df_fit_all['stationid'].unique()\n",
    "    print(f\"\\n[DIAGNOSTIC] Stations after Fitting (Step 3): {len(stations_in_fit)}\")\n",
    "    print(f\"[DIAGNOSTIC] Station loss from QC to Fit: {len(stations_in_qc) - len(stations_in_fit)}\")\n",
    "    \n",
    "    # 保存诊断报告\n",
    "    diagnostic_file = f'diagnostic_qc_to_fit_{a}_{b}_{c}_{d}.csv'\n",
    "    \n",
    "    df_fit_all.to_csv(f'3.fit_proba_modified_q50_iqr_{a}_{b}_{c}_{d}.csv', index=False)\n",
    "    \n",
    "    # 步骤5: 生成曲线\n",
    "    print(\"Step 5: Generating hypsometric curves...\")\n",
    "    df_hypso = HypsometricCurveGenerator.generate_curves(df_fit_all)\n",
    "    \n",
    "    if df_hypso is None or len(df_hypso) == 0:\n",
    "        print(f\"No hypsometric curves generated for {a}_{b}_{c}_{d}\")\n",
    "        return\n",
    "    \n",
    "    df_hypso.to_csv(f'4.hypso_med_modified_q50_iqr_{a}_{b}_{c}_{d}.csv', index=False)\n",
    "    \n",
    "    # 步骤6: 验证\n",
    "    print(\"Step 6: Validating model...\")\n",
    "    validator = ModelValidator()\n",
    "    df_width = common_data['df_width']\n",
    "    \n",
    "    # 【修改】传入 skip_width_filter 参数\n",
    "    df_results = validator.validate(\n",
    "        df_hypso, df_width,\n",
    "        '/home/xj/device5/202411-SWAP/northchina/landsat/4-validation/observation',\n",
    "        df_fit_all,\n",
    "        skip_width_filter=skip_width_filter  # 传递参数\n",
    "    )\n",
    "    \n",
    "    if df_results is None or len(df_results) == 0:\n",
    "        print(f\"No validation results for {a}_{b}_{c}_{d}\")\n",
    "        return\n",
    "    \n",
    "    df_results.to_csv(f'5.q_kge_med_modified_q50_iqr_{a}_{b}_{c}_{d}.csv', index=False)\n",
    "    \n",
    "    print(f\"Configuration {a}_{b}_{c}_{d} completed!\")\n",
    "    gc.collect()\n",
    "\n",
    "# ============================================================================\n",
    "# 配置生成函数\n",
    "# ============================================================================\n",
    "def generate_configs():\n",
    "    \"\"\"\n",
    "    生成所有有效的配置组合\n",
    "    规则: noqa只和node组合，qaloose和qastrict可以和所有A选项组合\n",
    "    \n",
    "    B选项现在是IQR倍数: '1.0', '1.5', '2.0', '2.5', '3.0', '4.0'\n",
    "    注意: \n",
    "    - 当a='node'时，遍历所有b选项，正常进行宽度筛选\n",
    "    - 当a='datemean'时，也遍历所有b选项（用于验证阶段），但拟合阶段不做宽度筛选\n",
    "    \"\"\"\n",
    "    A_options = ['node']\n",
    "    B_options = ['1.0','1.5']  # IQR倍数\n",
    "    C_options = ['noqa','qaloose']\n",
    "    D_options = ['VersionD']\n",
    "    \n",
    "    configs = []\n",
    "    \n",
    "    for a in A_options:\n",
    "        for b in B_options:\n",
    "            for c in C_options:\n",
    "                for d in D_options:\n",
    "                    # noqa只和node组合\n",
    "                    if c == 'noqa' and a != 'node':\n",
    "                        continue\n",
    "                    configs.append((a, b, c, d))\n",
    "    \n",
    "    return configs\n",
    "\n",
    "# ============================================================================\n",
    "# 主程序\n",
    "# ============================================================================\n",
    "def main():\n",
    "    \"\"\"主程序流程\"\"\"\n",
    "    import time\n",
    "    total_start = time.time()\n",
    "    \n",
    "    print(\"Loading common data...\")\n",
    "    df_l8 = pd.read_csv('../2-preprocess/1.north_glow_datemean_width_timeseries.csv')\n",
    "    df_comid = pd.read_csv('../2-preprocess/4.q50_weighted_slope.csv')[['stationid', 'COMID']]\n",
    "    df_attrs = pd.read_csv('../2-preprocess/4.q50_weighted_slope.csv')\n",
    "    df_width = pd.read_csv('../2-preprocess/1.north_glow_datemean_width_timeseries.csv')\n",
    "    df_width['date'] = pd.to_datetime(df_width['date'])\n",
    "    \n",
    "    common_data = {\n",
    "        'df_l8': df_l8,\n",
    "        'df_comid': df_comid,\n",
    "        'df_attrs': df_attrs,\n",
    "        'df_width': df_width\n",
    "    }\n",
    "    \n",
    "    # 生成有效配置\n",
    "    configs = generate_configs()\n",
    "    \n",
    "    print(f\"\\nTotal configurations to run: {len(configs)}\")\n",
    "    print(\"Configurations:\")\n",
    "    for cfg in configs:\n",
    "        print(f\"  {cfg}\")\n",
    "    \n",
    "    # 运行所有配置\n",
    "    for a, b, c, d in configs:\n",
    "        start = time.time()\n",
    "        run_configuration(a, b, c, d, common_data)\n",
    "        print(f\"Time for ({a}, {b}, {c}, {d}): {time.time() - start:.2f}s\")\n",
    "    \n",
    "    # 生成箱型图\n",
    "    print(\"\\nGenerating boxplot comparisons...\")\n",
    "    metrics = ['kge', 'nse', 'nrmse']\n",
    "    data_dict = {metric: [] for metric in metrics}\n",
    "    labels = []\n",
    "    \n",
    "    for a, b, c, d in configs:\n",
    "        file = f'5.q_kge_med_modified_q50_iqr_{a}_{b}_{c}_{d}.csv'\n",
    "        if os.path.exists(file):\n",
    "            df = pd.read_csv(file)\n",
    "            label = f'{a}_{b}_{c}_{d}'\n",
    "            labels.append(label)\n",
    "            for metric in metrics:\n",
    "                station_metrics = df.groupby('stationid')[metric].mean().values\n",
    "                data_dict[metric].append(station_metrics)\n",
    "        \n",
    "    for metric in metrics:\n",
    "        if data_dict[metric]:\n",
    "            fig, ax = plt.subplots(figsize=(14, 6))\n",
    "            ax.boxplot(data_dict[metric], labels=labels)\n",
    "            ax.set_title(f'{metric.upper()} Boxplot Comparison')\n",
    "            ax.set_xlabel('Configuration (A_B_C_D)')\n",
    "            ax.set_ylabel(metric.upper())\n",
    "            plt.xticks(rotation=45, ha='right')\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(f'boxplot_{metric}.png', dpi=150)\n",
    "            plt.close()\n",
    "    \n",
    "    print(f\"\\nTotal time: {time.time() - total_start:.2f}s\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
